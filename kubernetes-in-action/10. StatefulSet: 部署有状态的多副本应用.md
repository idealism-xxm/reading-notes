## 了解 StatefulSet `P289` 

### 对比 StatefulSet 和 ReplicaSet `P289`

#### 通过宠物与牛的类比来理解有状态 `P289`

可以把我们的应用看作宠物或牛。 StatefulSet 最初被称为 PetSet ，这个名字来源于宠物与牛的类比。 `P289`

对于无状态的应用实例来说，行为非常像农场里的牛。一个实例挂掉之后并没什么影响，可以创建一个新实例，而让用户完全无感知。 `P289`

有状态的应用的一个实例更像一个宠物。若一只宠物死掉，不能买到一只完全一样的，而不让用户感知到。若要替换掉这只宠物，需要找到一只行为举止与之前完全一致的宠物。对应用来说，意味着新的实例需要拥有跟旧的实例完全一致的状态和标识。 `P289`

#### StatefulSet 与 ReplicaSet 的对比 `P289`

无状态的 Pod 任何时候都可以被一个全新的 Pod 替换。而一个有状态的 Pod 挂掉之后，这个 Pod 实例需要在别的节点上重建，但是新的实例必须与被替换的实例拥有相同的名称、网络标识和状态。 `P289`

StatefulSet 保证了 Pod 在重新调度后保留它们的标识和状态。它会让你方便地扩容、缩容，可以指定期望的副本个数。它也是依据 Pod 模版创建 Pod 的，但创建的 Pod 副本并不是完全一样的。每个 Pod 都可以拥有一组独立的数据卷（持久化状态）而有所区别。并且 Pod 的名字都是有规律的（固定的），而不是每个新 Pod 都随机获取一个名字。 `P289`

### 提供稳定的网络标识 `P290`

一个 StatefulSet 创建的每个 Pod 都有一个从零开始的顺序索引，这个会体现在 Pod 的名称和主机名上，同样还会体现在 Pod 对应的固定存储上。 `P290`

![图 10.5 StatefulSet 创建的 Pod 拥有规则的名称和主机名](img/chapter10/图%2010.5%20StatefulSet%20创建的%20Pod%20拥有规则的名称和主机名.png)

#### 控制服务介绍 `P290`

一个 StatefulSet 通常要求创建一个用来记录每个 Pod 网络标记的 headless Service 。通过这个 Service ，每个 Pod 将拥有独立的 DNS 记录，这样集群里它的伙伴或者客户端可以通过主机名方便地找到它。比如：一个属于 `default` 命名空间，名为 `foo` 的控制服务，它的一个 Pod 名称为 `A-0` ，那么可以通过下面的完整域名来访问它： `a-0.foo.default.svc.cluster.local` 。 `P290`

同时，也可以通过 DNS 服务，查找域名 `foo.default.svc.cluster.local` 对应的所有 SRV 记录，获取一个 StatefulSet 中所有 Pod 的名称。 `P290`

#### 替换消失的宠物 `P290`

当 StatefulSet 管理的一个 Pod 实例消失后， StatefulSet 会保证重启一个新的 Pod 实例替换它，并且这个新的 Pod 拥有与之前 Pod 完全一致的名称和主机名。 `P291`

![图 10.6 StatefulSet 是用表示完全一致的新 Pod 替换](img/chapter10/图%2010.6%20StatefulSet%20是用表示完全一致的新%20Pod%20替换.png)

#### 扩缩容 StatefulSet `P291`

扩容一个 StatefulSet 会使用下一个还没用到的顺序索引值创建一个新的 Pod 实例。 `P291`

缩容一个 StatefulSet 会最先删除最高索引值的实例，所以缩容的结果是可预知的。 `P292`

![图 10.7 缩容一个 StatefulSet 将会最先删除最高索引值的实例](img/chapter10/图%2010.7%20缩容一个%20StatefulSet%20将会最先删除最高索引值的实例.png)

因为 StatefulSet 缩容任何时候只会操作一个 Pod 实例，所以有状态应用的缩容不会很迅速。 `P292`

StatefulSet 在有实例不健康的情况下是不允许做缩容操作的。若一个实例是不健康的，而这时再缩容一个实例的话，也就意味着实际上同时失去了两个集群成员。 `P292`

### 为每个有状态实例提供稳定的专属存储 `P292`

有状态的 Pod 的存储必须是持久的，并且与 Pod 解耦（[06. 卷：将磁盘挂载到容器](06.%20卷：将磁盘挂载到容器.md) 中介绍的持久卷和持久卷声明可以为 Pod 提供持久化存储）。 `P292`

因为持久卷声明与持久卷是一对一的关系，所以每个 StatefulSet 的 Pod 都需要关联到不同的持久卷声明，与独自的持久卷相对应。 `P292`

#### 在 Pod 模版中添加卷声明模版 `P293`

像 StatefulSet 创建 Pod 一样， StatefulSet 也需要创建持久卷声明。所以一个 StatefulSet 可以拥有一个或多个卷声明模版，这些持久卷会在创建 Pod 前创建出来，绑定到一个 Pod 实例上。 `P293`

![图 10.8 一个 StatefulSet 创建 Pod 和持久卷声明](img/chapter10/图%2010.8%20一个%20StatefulSet%20创建%20Pod%20和持久卷声明.png)

#### 持久卷的创建与删除 `P293`

扩容 StatefulSet 增加一个副本数时，会创建两个或更多的 API 对象（一个 Pod 和与之关联的一个或多个持久卷声明）。但是对缩容来说，则只会删除一个 Pod ，而留下之前创建的声明。如果需要释放特定的持久卷，需要手动删除对应的持久卷声明。 `P293`

#### 重新挂载持久卷声明到相同 Pod 的新实例上 `P293`

因为缩容 StatefulSet 时会保留持久卷声明，所以在随后的扩容操作中，新的 Pod 实例会使用绑定在持久卷上的相同声明和其上的数据，新的 Pod 实例会运行到与之前完全一致的状态。 `P293`

![图 10.9 StatefulSet 缩容时不会删除持久卷声明](img/chapter10/图%2010.9%20StatefulSet%20缩容时不会删除持久卷声明.png)

### StatefulSet 的保障 `P294`

StatefulSet 不仅拥有稳定的标记和独立的存储，它的 Pod 还有其他的一些保障。 `P294`

如果 StatefulSet 存在两个完全一致的 Pod ，那么这两个 Pod 会绑定到相同的存储，所以这两个相同标记的进程会同时写相同的文件，有违 StatefulSet 的定义与作用。 `P294`

Kubernetes 必须保证两个拥有相同标记和绑定相同持久卷声明的有状态的 Pod 实例不会同时运行。一个 StatefulSet 必须保证有状态的 Pod 实例的 `at-most-one` 语义。即一个 StatefulSet 必须在准确确认一个 Pod 不再运行后，才会去创建它的替换 Pod 。 `P295`

## 使用 StatefulSet `P295`

### 创建应用和容器镜像 `P295`

我们可以使用以下 `kubia-pet-image/app.js` 文件来处理与持久化存储相关的请求，并用于构建镜像 `idealism/kubia-pet` 。 `P295`

```javascript
...
const dataFile = "/var/data/kubia.txt";
...
var handler = function(request, response) {
    if (request.method == 'POST') {
        var file = fs.createWriteStream(dataFile);
        file.on('open', function (fd) {
            request.pipe(file);
            console.log("New data has been received and stored.");
            response.writeHead(200);
            response.end("Data stored on pod " + os.hostname() + "\n");
        });
    } else {
        var data = fileExists(dataFile) ? fs.readFileSync(dataFile, 'utf8') : "No data posted yet";
        response.writeHead(200);
        response.write("You've hit " + os.hostname() + "\n");
        response.end("Data stored on this pod: " + data + "\n");
    }
};
...
```

当该应用接收到一个 POST 请求时，它把请求中的 body 数据内容写入 `/var/data/kubia.txt` 文件中。而在收到 GET 请求时，它返回主机名和文件中的内容。 `P296`

### 通过 StatefulSet 部署应用 `P296`

为了部署应用，需要创建两个（或三个）不同类型的对象： `P296`

- 存储你数据文件的持久卷（当集群不支持持久卷的动态供应时，需要手动创建）
- StatefulSet 必须的一个控制 Service
- StatefulSet 本身

#### 创建持久化存储卷 `P296`

我们使用的是 minikube ，可以通过 `persistent-volumes-hostpath.yaml` 创建三个持久卷。

```yaml
# 遵循 v1 版本的 Kubernetes API
apiVersion: v1
# 资源类型为 List ，表示我们可以使用一个 yaml 文件创建多个资源对象
# 效果和三横线 (---) 区分多个资源一致
kind: List
items:
  # 第一个资源遵循 v1 版本的 Kubernetes API
  - apiVersion: v1
    # 资源类型为 PersistentVolume
    kind: PersistentVolume
    metadata:
      # PV 的名称
      name: pv-a
    spec:
      # 该 PV 的大小为 1MB
      capacity:
        storage: 1Mi
      # 访问模式列表
      accessModes:
        # 可以被单个客户端挂载为读写模式
        - ReadWriteOnce
      # 当声明被释放后， PV 将被回收
      persistentVolumeReclaimPolicy: Recycle
      # 指定 storageClassName 为 standard
      storageClassName: standard
      # minikube 下指定节点上的路径为一个 hostPath 卷
      hostPath:
        path: /tmp/pv-a
  # 第二个资源遵循 v1 版本的 Kubernetes API
  - apiVersion: v1
    # 资源类型为 PersistentVolume
    kind: PersistentVolume
    metadata:
      # PV 的名称
      name: pv-b
    ...
  # 第三个资源遵循 v1 版本的 Kubernetes API
  - apiVersion: v1
    # 资源类型为 PersistentVolume
    kind: PersistentVolume
    metadata:
      # PV 的名称
      name: pv-c
    ...
```

**注意**：这里需要指定 `storageClassName` 字段值为 standard ，要不然后续通过模版创建的 PVC 无法成功与这些 PV 绑定

#### 创建控制 Service `P297`

我们可以使用如下描述文件 `kubia-service-headless.yaml` 创建一个 Service ，用于在有状态的 Pod 之间提供网络标识。 `P297`

```yaml
# 第一个资源遵循 v1 版本的 Kubernetes API
apiVersion: v1
# 资源类型为 Service
kind: Service
metadata:
  # Service 的名称
  name: kubia
spec:
  # Stateful 的控制 Service 必须是 headless 模式
  clusterIP: None
  # 所有标签为 app=kubia 的 Pod 都属于这个 Service
  selector:
    app: kubia
  # 该服务可用的端口
  ports:
    # 第一个可用端口的名字
    - name: http
      # 可用端口为 80
      port: 80
```

#### 创建 StatefulSet `P299`

我们可以使用如下描述文件 `kubia-statefulset.yaml` 创建 StatefulSet 。 `P299`

```yaml
# 第一个资源遵循 apps/v1 版本的 Kubernetes API
apiVersion: apps/v1
# 资源类型为 StatefulSet
kind: StatefulSet
metadata:
  # StatefulSet 的名称
  name: kubia
spec:
  # StatefulSet 的控制 Service 的名字
  serviceName: kubia
  selector:
    # 需要匹配以下指定的标签
    matchLabels:
      app: kubia
  # 最大副本数为 2
  replicas: 2
  # Pod 模版
  template:
    # Pod 含有 app=kubia 标签
    metadata:
      labels:
        app: kubia
    spec:
      # Pod 含有一个名为 kubia 的容器
      containers:
        - name: kubia
          image: idealism/kubia-pet
          # 该容器的端口列表
          ports:
            # 第一个可用的端口为 8080 ，名字是 http
            - name: http
              containerPort: 8080
          # 该容器挂载列表
          volumeMounts:
            # 将名为 data 的卷挂载到容器的 /var/data
            - name: data
              mountPath: /var/data
  # 持久卷声明模版
  volumeClaimTemplates:
    - metadata:
        # 名为 data
        name: data
      spec:
        resources:
          # 申请 1MB 的空间
          requests:
            storage: 1Mi
        # 访问模式列表
        accessModes:
          # 可以被单个客户端挂载为读写模式
          - ReadWriteOnce
```

`kubectl get pods`: 可以发现只启动了一个 Pod ，后一个 Pod 会在前一个 Pod 运行并且处于就绪状态后创建，因为状态明确的集群应用对同时有两个集群成员启动引起的竞争情况非常敏感 `P299`

### 使用 Pod `P301`

前面已经介绍过了直接访问 Pod ，这次我们通过 API 服务器作为代理访问 Pod 。 `P301`

`kubectl proxy`: 开启代理，让其帮我们隐藏授权和 SSL 证书 `P301`

`curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/`: 通过代理访问指定的 Pod ，可得到 Pod 的返回 `P301`

```shell script
You've hit kubia-0
Data stored on this pod: No data posted yet
```

![图 10.10 通过 kubectl 代理和 API 服务器代理来与一个 Pod 通信](img/chapter10/图%2010.10%20通过%20kubectl%20代理和%20API%20服务器代理来与一个%20Pod%20通信.png)

`curl -X POST -d "Accessing kubia-0" localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/`: 将 `Accessing kubia-0` 通过代理写入 Pod `P302`

```shell script
Data stored on pod kubia-0
```

`curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/`: 此时可以发现 Pod 返回了我们刚刚写入的数据 `P302`

```shell script
You've hit kubia-0
Data stored on this pod: Accessing kubia-0
```

如果我们删除一个有状态 Pod ，那么 StatefulSet 会重新创建一个具有相同名称的新 Pod ，新 Pod 可能会被调度到集群中的任何一个节点上。 `P303`

![图 10.11 一个有状态 Pod 会被重新调度到新的节点，但会保留它的名称、主机名和存储](img/chapter10/图%2010.11%20一个有状态%20Pod%20会被重新调度到新的节点，但会保留它的名称、主机名和存储.png)

#### 通过一个普通的非 headless 的 Service 暴露 StatefulSet 的 Pod `P304`

我们可以使用如下描述文件 `kubia-service-public.yaml` 创建一个普通 Service 供我们通过 API 服务器访问有状态 Pod 。 `P304`

```yaml
# 遵循 v1 版本的 Kubernetes API
apiVersion: v1
# 资源类型为 Service
kind: Service
metadata:
  # Service 的名称
  name: kubia-public
spec:
  # 该服务可用的端口
  ports:
    # 第一个可用端口的名字
    - name: http
      # 可用端口为 80
      port: 80
      # 服务将连接转发到容器的 8080 端口
      targetPort: 8080
  # 具有 app=kubia 标签的 pod 都属于该服务
  selector:
    app: kubia
```

`curl localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/`: 通过 API 服务器访问集群内部的服务 `P304`

## 在 StatefulSet 中发现伙伴节点 `P305`

集群应用中很重要的一个需求是伙伴节点彼此能发现——这样才可以找到集群中的其他成员。一个 StatefulSet 中的成员需要很容易地找到其他的所有成员。 `P305`

### 介绍 SRV 记录 `P305`

SRV 记录用来指向提供指定服务的服务器的主机名和端口号。 Kubernetes 通过一个 headless Service 创建 SRV 记录来指向 Pod 的主机名。 `P305`

`kubectl run -ti srvlookup --image=tutum/dnsutils --rm --restart=Never -- dig SRV kubia.default.svc.cluster.local`: 在容器 srvlookup 中运行 `dig` 命令，列出有状态 Pod 的 SRV 记录。 `P305`
- `--restart=Never`: 表示这个一个一次性 Pod
- `--rm`: 表示在终止后会立即删除

```shell script
...
;; ANSWER SECTION:
kubia.default.svc.cluster.local. 30 IN  SRV     0 20 80 kubia-1.kubia.default.svc.cluster.local.
kubia.default.svc.cluster.local. 30 IN  SRV     0 20 80 kubia-0.kubia.default.svc.cluster.local.
kubia.default.svc.cluster.local. 30 IN  SRV     0 20 80 10-88-0-3.kubia.default.svc.cluster.local.
kubia.default.svc.cluster.local. 30 IN  SRV     0 20 80 10-88-0-4.kubia.default.svc.cluster.local.

;; ADDITIONAL SECTION:
10-88-0-4.kubia.default.svc.cluster.local. 30 IN A 10.88.0.4
kubia-1.kubia.default.svc.cluster.local. 30 IN A 10.244.2.2
10-88-0-3.kubia.default.svc.cluster.local. 30 IN A 10.88.0.3
kubia-0.kubia.default.svc.cluster.local. 30 IN A 10.244.3.2
...
```

上面的 `ANSWER SECTION` 显示了指向后台 headless Service 的 SRV 记录， `ADDITIONAL SECTION` 显示每个 Pod 都拥有独立的记录。 `P306`

当一个 Pod 要获取一个 StatefulSet 里的其他 Pod 列表时，就触发一次 SRV DNS 查询即可。 `P306`

**注意**：返回的 SRV 记录顺序是随机的，因为它们拥有相同的优先级。 `P306`

### 通过 DNS 实现伙伴间彼此发现 `P306`

如果我们想让客户端通过一次请求就能获取所有 Pod 中存储的数据，那么就需要使用 Pod 间通信。可以使用如下代码，使用 StatefulSet 和 SRV 记录来实现这个功能。 `P306`

```javascript
...
const dns = require('dns');

const dataFile = "/var/data/kubia.txt";
const serviceName = "kubia.default.svc.cluster.local";
...
function httpGet(reqOptions, callback) {
    return http.get(reqOptions, function(response) {
        var body = '';
        response.on('data', function(d) { body += d; });
        response.on('end', function() { callback(body); });
    }).on('error', function(e) {
        callback("Error: " + e.message);
    });
}

var handler = function(request, response) {
    if (request.method == 'POST') {
        var file = fs.createWriteStream(dataFile);
        file.on('open', function (fd) {
            request.pipe(file);
            response.writeHead(200);
            response.end("Data stored on pod " + os.hostname() + "\n");
        });
    } else {
        response.writeHead(200);
        if (request.url == '/data') {
            var data = fileExists(dataFile) ? fs.readFileSync(dataFile, 'utf8') : "No data posted yet";
            response.end(data);
        } else {
            response.write("You've hit " + os.hostname() + "\n");
            response.write("Data stored in the cluster:\n");
            // 通过 DNS 查询获取 SRV 记录
            dns.resolveSrv(serviceName, function (err, addresses) {
                if (err) {
                    response.end("Could not look up DNS SRV records: " + err);
                    return;
                }
                var numResponses = 0;
                if (addresses.length == 0) {
                    response.end("No peers discovered.");
                } else {
                    // 与 SRV 记录对应的每个 Pod 通信获取其数据
                    addresses.forEach(function (item) {
                        var requestOptions = {
                            host: item.name,
                            port: port,
                            path: '/data'
                        };
                        httpGet(requestOptions, function (returnedData) {
                            numResponses++;
                            response.write("- " + item.name + ": " + returnedData + "\n");
                            if (numResponses == addresses.length) {
                                response.end();
                            }
                        });
                    });
                }
            });
        }
    }
};

var www = http.createServer(handler);
www.listen(8080);
```

此时我们可以把 `kubia-statefulset.yaml` 中的镜像 `idealism/kubia-pet` 换成 `idealism/kubia-pet-peers` ，并将副本数改为 3 ，然后运行 `kubectl apply -f kubia-statefulset.yaml` 即可使用最新的镜像替换原有的 Pod 。此时我们再发送一个 GET 请求，服务器就会触发一次 headless kubia 服务的 SRV 记录查询，然后发送 GET 请求到服务背后的每一个 Pod ，然后返回所有节点和它们的数据信息的列表。 `P308`

![图 10.12 简单的分布式数据存储服务的操作流程](img/chapter10/图%2010.12%20简单的分布式数据存储服务的操作流程.png)

`kubectl delete pod kubia-0 --force --grace-period 0`: 可强制删除一个 Pod ，让 API 服务器不用等待 kubelet 确认其是否已经不再运行。 `P313`

**警告**：除非你确认节点不再运行或者不再可访问（永远不会再可访问），否则不要强制删除有状态的 Pod 。 `P313`
