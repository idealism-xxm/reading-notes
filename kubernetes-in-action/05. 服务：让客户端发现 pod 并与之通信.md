#### 简介 `P121`

pod 通常需要对来自集群内部其他 pod ，以及来自集群外部的客户端的 HTTP 请求作出响应，所以需要一种寻找其他 pod 的方法来使用其他 pod 提供的服务。 `P121`

在 Kubernetes 中通过服务 (service) 解决以下问题： `P121`

- pod 是短暂的： pod 随时启动和关闭
- Kubernetes 在 pod 启动前会给已经调度到节点上的 pod 分配 IP 地址：客户端不能提前知道 pod 的 IP 地址
- 水平伸缩意味着多个 pod 可能提供相同的服务：每个 pod 都有自己的 IP 地址

#### 介绍服务 `P122`

Kubernetes 服务是一种为一组功能相同的 pod 提供但以不变的接入点的资源。当服务存在时，它的 IP 地址和端口不会改变。与服务建立的连接会被路由到提供该服务的任意一个 pod 上。 `P122`

![图 5.1 内部和外部客户端通常通过服务连接到 pod](img/chapter05/图%205.1%20内部和外部客户端通常通过服务连接到%20pod.png)

##### 创建服务 `P123`

服务使用标签选择器（[03. pod: 运行于 Kubernetes 中的容器](03.%20pod%3A%20运行于%20Kubernetes%20中的容器.md) 中介绍过标签选择器及使用方式）来指定属于同一组的 pod 。 `P123`

![图 5.2 标签选择器决定哪些 pod 属于服务](img/chapter05/图%205.2%20标签选择器决定哪些%20pod%20属于服务.png)

**通过 kubectl expose 创建服务** `P124`

创建服务的最简单的方法就是通过 `kubectl expose` ，在 [02. 开始使用 Kubernetes 和 Docker](02.%20开始使用%20Kubernetes%20和%20Docker.md) 中就使用该方法创建服务来暴露 `Deployment` 。 `P124`

**通过 YAML 描述文件来创建服务** `P124`

为了将创建服务，我们需要使用以下 `kubia-svc.yaml` 描述文件进创建。

```yaml
# 遵循 v1 版本的 Kubernetes API
apiVersion: v1
# 资源类型为 Service
kind: Service
metadata:
  # Service 的名称
  name: kubia
spec:
  # 该服务可用的端口
  ports:
    # 第一个可用端口的名字
    - name: http
      # 可用端口为 80
      port: 80
      # 服务将连接转发到容器的 8080 端口
      targetPort: 8080
    # 第二个可用端口的名字
    - name: https
      # 可用端口为 443
      port: 443
      # 服务将连接转发到容器的 8443 端口
      targetPort: 8443
  # 具有 app=kubia 标签的 pod 都属于该服务
  selector:
    app: kubia
```

`kubectl create -f kubia-svc.yaml`: 创建服务

`kubectl get services`: 查看当前所有服务

```shell script
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP          12d
kubia        ClusterIP   10.111.241.144   <none>        80/TCP,443/TCP   5s
```

可以发现刚刚启动的服务已经被分配了一个内部集群 IP ，并且对外暴露了两个端口。服务的主要目标就是使集群内部的其他 pod 可以访问当前这组 pod ，但通常也希望对外暴露服务。 `P125`

**从集群内部测试服务** `P125`

可以通过以下三种方式向服务发送请求： `P125`

- 创建一个 pod ，它将请求发送到服务的集群 IP 并记录响应。可以通过 `kubectl logs` 查看 pod 日志检查服务的响应
- 使用 `ssh` 远程登录到其中一个 Kubernetes 节点上，然后使用 `curl` 命令
- 通过 `kubectl exec` 命令在一个已经存在的 pod 中执行 `curl` 命令

**在运行的容器中远程执行命令** `P125`

`kubectl exec kubia-9495d9bf5-2mmv2 -- curl -s 10.111.241.144`: 在 pod `kubia-9495d9bf5-2mmv2` 运行命令 `curl -s 10.111.241.144`

`--` 代表 kubectl 命令项的结束，在 `--` 之后的内容是指在 pod 内部需要执行的命令。如果需要执行的命令没有以 `-` 开始的参数，那么 `--` 不是必须的。 `P125`

![图 5.3 使用 kubectl exec 在一个 pod 中运行 curl 命令](img/chapter05/图%205.3%20使用%20kubectl%20exec%20在一个%20pod%20中运行%20curl%20命令.png)

**配置服务上的会话亲和性** `P126`

如果希望特定客户端产生的所有请求每次都指向同一个 pod ，可以设置服务的 `spec.sessionAffinity` 属性为 `ClientIP` ，而不是默认值 `None` 。 `P127`

```yaml
...
spec:
  sessionAffinity: ClientIP
  ...
```

这种方式会使服务代理将来自同一个客户端 IP 的所有请求转发至同一个 pod 。 Kubernetes 仅支持两种形式的会话亲和性服务： `None` 和 `ClientIP` 。 `P127`

**同一个服务暴露多个端口** `P127`

我们在前面已将创建了暴露多个端口的服务，这样通过一个集群 IP ，使用一个服务就可以将多个端口全部暴露出来。 `P127`

**注意**：在创建一个有多个端口的服务的时候，必须给每个端口指定名字。 `P127`

**注意**：标签选择器应用于整个服务，不能对每个端口做单独的配置。如果不同的 pod 有不同的端口映射关系，需要创建两个服务。 `P128`

**使用命名的端口** `P128`

我们可以将 pod 端口定义改为如下形式： 

```yaml
...
kind: Pod
spec:
  containers:
    - name: kubia
      ports:
        # 应用监听端口 8080 ，并命名为 http
        - name: http
          containerPort: 8080
        # 应用监听端口 8443 ，并命名为 https
        - name: https
          containerPort: 8443
```

然后我们就可以将在服务中引用命名的端口： 

```yaml
...
kind: Service
spec:
  ports:
    - name: http
      port: 80
      targetPort: 8080
    - name: https
      port: 443
      targetPort: https
```

采用命名端口的方式可以使得更换 pod 端口时无须更改服务的 `spec` ，并且不同的 pod 可以使用不同的端口。 `P129`

##### 服务发现 `P129`

现在可以通过一个单一稳定的 IP 地址访问到 pod ，但是还没法让客户端 pod 知道服务的 IP 和端口，所以我们需要配置进行发现服务。 `P129`

**通过环境变量发现服务** `P129`

在 pod 开始运行时， Kubernetes 会初始化一系列环境变量指向现在存在的服务。如果 pod 先于服务启动，那么可以先把这些 pod 删除，等待 `Development` 自动创建新的 pod ，这样所有 pod 都能拥有现存服务的的相关环境变量了。 `P129`

`kubectl exec kubia-9495d9bf5-4jbtf env`: 查看指定 pod 的环境变量，可以发现其中有 `kubia` 和 `kubernetes` 服务的 IP 地址和端口号的环境变量

```shell script
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=kubia-9495d9bf5-4jbtf
...
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBIA_SERVICE_HOST=10.111.232.152
KUBIA_SERVICE_PORT=80
KUBIA_SERVICE_PORT_HTTP=80
KUBIA_SERVICE_PORT_HTTPS=443
...
```

**注意**：环境变量中，服务名作为前缀时，所有字母变为变为大写，且服务名称中的 `-` 将被转换为 `_`  `P130`

**通过 DNS 发现服务** `P130`

命名空间 `kube-system` 下有几个以 `coredns` 为前缀的 pod 。这些 pod 运行 DNS 服务，在集群中的其他 pod 都被配置成使用其作为 dns (Kubernetes 通过修改每个容器的 `etc/resolv.conf` 文件实现 ) 。运行在 pod 上的进程 DNS 查询都会被 Kubernetes 自身的 DNS 服务器响应，该服务器知道系统中运行的所有服务。 `P130`

**注意**： pod 是否使用内部的 DNS 服务器是根据 pod 的 `spec.dnsPolicy` 属性决定的。 `P130` 

每个服务从内部 DNS 服务器中获得一个 DNS 条目，客户端的 pod 在知道服务名称的情况下可以通过全限定域名 (FQDN) 来访问。 `P131`

**通过 FQDN 连接服务** `P131`

我们可以通过 `kubia.default.svc.cluster.local` 来访问 `kubia` 服务。其中 `kubia` 对应于服务名称， `default` 表示服务所在的命名空间， `svc.cluster.local` 是所在集群本地服务名称中使用的可配置集群域后缀。 `P131`

**注意**：客户端仍然必须知道服务的端口号。如果服务没有使用标准端口号，那么客户端仍然需要从环境变量中获取端口号。 `P131`

如果服务和客户端在同一个命名空间下，那么可是直接使用服务名（例如： `kubia` ）指代服务。 `P131`

**在 pod 容器中运行 shell** `P131`

`kubectl exec -ti <pod-name> bash`: 可以在一个 pod 容器上运行 bash （也可指定其他形式的 shell ） `P131`

**无法 ping 通服务 IP 的原因** `P132`

服务的集群 IP 是一个虚拟 IP ，并且只有在与服务端口结合时才有意义。将在后续文章中详细讲解。 `P132`

#### 连接集群外部的服务 `P132`

在集群中运行的客户端 pod 可以像连接到内部服务一样连接到外部服务，这样做可以充分利用负载均衡和服务发现。 `P132`

##### 介绍服务 endpoints `P133`

服务并不是与 pod 直接相连的，而是通过 Endpoints 资源与 pod 连接。 Endpoints 资源就是暴露一个服务的 IP 地址和端口列表，和其他 Kubernetes 资源一样。 `P133`

`kubectl get endpoints kubia`: 查看 kubia 的 endpoints 基本信息

```shell script
NAME    ENDPOINTS                                                  AGE
kubia   10.88.0.2:8443,10.88.0.3:8443,10.88.0.3:8443 + 3 more...   3d
```

服务中在 `spec.selector` 定义了 pod 选择器，但是在重定向传入连接时不会直接使用它。选择器用于构建 IP 和端口列表，然后存储在 Endpoints 资源中。当客户端连接到服务时，服务代理选择这些 IP 和端口对中的一个，并将传入连接重定向到该位置监听的服务器。 `P133`

##### 手动配置服务的 endpoints

如果将服务的 endpoints 与服务解耦，那么就可以手动配置和更新它们。 `P133`

如果创建了不包含 pod 选择器的服务， Kubernetes 将不会创建 Endpoints 资源。这样就需要创建 Endpoints 资源来指定该服务的 endpoints 列表。 `P133`

**创建没有 pod 选择器的服务** `P133`

使用以下描述文件 `external-service.yaml` 可以创建一个不指定 pod 选择器的服务。

```yaml
# 遵循 v1 版本的 Kubernetes API
apiVersion: v1
# 资源类型为 Service
kind: Service
metadata:
  # Service 的名称
  name: external-service
spec:
  # 该服务可用的端口
  ports:
    # 第一个可用端口的名字
    - name: http
      # 可用端口为 80
      port: 80
      targetPort: http
    # 第二个可用端口的名字
    - name: https
      # 可用端口为 443
      port: 443
      targetPort: https
```

使用以下描述文件 `external-service-endpoints.yaml` 可以创建一个 Endpoints 资源。

```yaml
# 遵循 v1 版本的 Kubernetes API
apiVersion: v1
# 资源类型为 Endpoints
kind: Endpoints
metadata:
  # Endpoints 的名称，与对应的 Service 名称一致
  name: external-service
# 该 Endpoints 的子集
subsets:
  # 第一个子集的地址信息
  - addresses:
      # 地址包含以下 ip 列表
      - ip: 11.11.11.11
      - ip: 22.22.22.22
    # 第一个子集的端口信息
    ports:
      # 每个 ip 可用的端口列表
      # 【注意】这个名字必须和服务端端口的名字对应
      - name: http
        port: 80
      - name: https
        port: 443
```

Endpoints 对象需要与服务具有相同的名称，并包含该服务将要重定向的目标 IP 地址和端口列表。当服务和 Endpoints 都创建后，服务就会自动使用对应当 Endpoints ，并能够像具有 pod 选择器那样当服务正常使用。 `P134`

![图 5.4 pod 关联到具有两个外部 endpoints 的服务上](img/chapter05/图%205.4%20pod%20关联到具有两个外部%20endpoints%20的服务上.png)

##### 为外部服务创建别名 `P135`

除了手动配置服务的 Endpoints 来代替公开外部服务的方法，还可以通过其完全限定域名 (FQDN) 来访问外部服务。 `P135`

**创建 `ExternalName` 类型的服务** `P135`

通过以下描述文件 `external-service-externalname.yaml` 可以创建一个 `ExternalName` 类型的服务，这个服务会将请求转发到 `spec.externalName` 指定的实际服务的完全限定域名。 `P135`

```yaml
# 遵循 v1 版本的 Kubernetes API
apiVersion: v1
# 资源类型为 Service
kind: Service
metadata:
  # Service 的名称
  name: external-service
spec:
  # Service 的类型为 ExternalName
  type: ExternalName
  # 这个服务将所有请求都转发到 someapi.somecompany.com
  externalName: leetcode-cn.com
  # 该服务可用的端口
  ports:
    # 第一个可用端口的名字
    - name: http
      # 可用端口为 80
      port: 80
      # 使用 ExternalName 时， targetPort 将被忽略
    # 第一个可用端口的名字
    - name: https
      # 可用端口为 443
      port: 443
      # 使用 ExternalName 时， targetPort 将被忽略
```

服务创建完成后， pod 可以通过 `external-service(.default.svc.cluster.local)` 域名（括号内的可不加）连接到外部服务，而不用使用外部服务的实际 FQDN 。这样允许修改服务的定义，并且在以后可以修改 `externalName` 指向到不同的服务，或者将类型变为 `ClusterIP` 并为服务创建 Endpoints 。 `P135`

`ExternalName` 服务仅在 DNS 级别实施——为服务创建了简单 `CNAME` DNS记录。因此，连接到服务的客户端将直接连接到外部服务，完全绕过服务代理，所以这类型的服务不会获得集群 IP 。 `P135`

**注意**： `CNAME` 记录指向完全限定的域名而不是 IP 地址。 `P136`

#### 将服务暴露给外部客户端 `P136`

![图 5.5 将服务暴露给外部客户端](img/chapter05/图%205.5%20将服务暴露给外部客户端.png)

有以下三种方式可以在外部访问服务：
- 将服务的类型设置成 `NodePort`
- 将服务的类型设置为 `LoadBalance`
- 创建一个 `Ingress` 资源

##### 使用 `NodePort` 类型的服务 `P137`

通过创建一个 `NodePort` 服务，可以让 Kubernetes 在其所有节点上保留一个端口（所有节点上都使用相同端口号），并将传入的连接转发给作为服务部分的 pod 。 `P137`

**创建 `NodePort` 类型的服务** `P137`

可以使用如下描述文件 `kubia-svc-nodeport.yaml` 创建一个 `NodePort` 类型的服务。

```yaml
# 遵循 v1 版本的 Kubernetes API
apiVersion: v1
# 资源类型为 Service
kind: Service
metadata:
  # Service 的名称
  name: kubia-nodeport
spec:
  # 指定服务类型为 NodePort
  type: NodePort
  # 该服务可用的端口
  ports:
    # 第一个可用端口的名字
    - name: http
      # 可用端口为 80
      port: 80
      # 服务将连接转发到容器的 8080 端口
      targetPort: 8080
      # 通过集群节点的 30000 端口可以访问该服务
      nodePort: 30000
    # 第二个可用端口的名字
    - name: https
      # 可用端口为 443
      port: 443
      # 服务将连接转发到容器的 8443 端口
      targetPort: 8443
      # 通过集群节点的 32767 端口可以访问该服务
      nodePort: 32767
  # 具有 app=kubia 标签的 pod 都属于该服务
  selector:
    app: kubia
```

`nodePort` 属性不是强制的，如果忽略就会随机选择一个端口。 `P137`

`kubectl get services kubia-nodeport`: 查看该服务的基础信息

```shell script
NAME               TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)                      AGE
kubia-nodeport     NodePort       10.111.59.156    <none>            80:30000/TCP,443:32767/TCP   2s
```

`PORT(S)` 列显示集群 IP 内部端口 (80, 443) 和节点端口 (30000, 32767) ，可通过 `10.111.59.156:80` 和 `<any-node-ip>:30000` 等访问服务。 `P138`

![图 5.6 外部客户端通过节点 1 或者节点 2 连接到 NodePort 服务](img/chapter05/图%205.6%20外部客户端通过节点%201%20或者节点%202%20连接到%20NodePort%20服务.png)

**使用 JSONPath 输出需要的信息**：通过指定 kubectl 的 JSONPath ，我们可以只输出需要的信息。例如： `kubectl get nodes -o jsonpath='{.items[*].status.addresses[0].address}'` 将输出所有节点的 IP 地址。

##### 通过负载均衡器将服务暴露出来 `P140`

负载均衡器拥有自己独一无二的可公开访问的 IP 地址，并将所有连接重定向到服务。 `P140`

**创建 `LoadBalance` 服务** `P140`

可以使用如下描述文件 `kubia-svc-loadbalancer.yaml` 创建一个 `LoadBalancer` 类型的服务。

```yaml
# 遵循 v1 版本的 Kubernetes API
apiVersion: v1
# 资源类型为 Service
kind: Service
metadata:
  # Service 的名称
  name: kubia-loadbalancer
spec:
  type: LoadBalancer
  # 该服务可用的端口
  ports:
    # 第一个可用端口的名字
    - name: http
      # 可用端口为 80
      port: 80
      # 服务将连接转发到容器的 8080 端口
      targetPort: 8080
    # 第二个可用端口的名字
    - name: https
      # 可用端口为 443
      port: 443
      # 服务将连接转发到容器的 8443 端口
      targetPort: 8443
  # 具有 app=kubia 标签的 pod 都属于该服务
  selector:
    app: kubia
```

使用 minikube 会发现服务的 `EXTERNAL-IP` 一直为 `<pending>` ，我们可以使用 minikube 自带的 `minikube tunnel` 命令可以完成暴露（[02. 开始使用 Kubernetes 和 Docker](02.%20开始使用%20Kubernetes%20和%20Docker.md) 介绍过相关处理及踩过的坑）。

![图 5.7 外部客户端连接一个 LoadBalancer 服务.png](img/chapter05/图%205.7%20外部客户端连接一个%20LoadBalancer%20服务.png)

`LoadBalancer` 类型的服务是一个具有额外的基础设施提供的负载均衡器 `NodePort` 服务。使用 `kubectl describe service kubia-loadbalancer` 命令可以发现该服务选择了一个节点端口。 `P141`

##### 了解外部连接的特性 `P142`

**了解并防止不必要的网络跳数**：当外部客户端通过节点端口连接到服务时，随机选择的 pod 并不一定在接收连接的同一节点上。可能需要额外的网络跳转才能到达 pod 。可配置 `Service.spec.externalTrafficPolicy` 的值为 `Local` 指定仅将外部通信重定向到接收连接的节点上运行的 pod 。
- 如果接收连接的节点上没有运行对应的 pod ，那么连接将挂起，所以需要确保负载均衡器将连接转发给至少具有一个 pod 的节点
- 假设有一个两节点三个 pod 的集群（节点 A 运行一个 pod ，节点 B 运行两个 pod），如果负载均衡器在两个节点间均匀分布连接，那么 pod 的负载分布不均衡

![图 5.8 使用 Local 外部流量策略的服务可能会导致 pod 的负载分布不均衡](img/chapter05/图%205.8%20使用%20Local%20外部流量策略的服务可能会导致%20pod%20的负载分布不均衡.png)

**客户端 IP 是不记录的**：当通过节点端口接收到连接时，由于对数据包执行了源网络地址转换 (SNAT) ，因此数据包对源 IP 将发生更改。如果配置 `Service.spec.externalTrafficPolicy` 的值为 `Local` ，那么将会保留客户端 IP ，因为在接收连接的节点和托管目标 pod 的节点之间没有额外的跳跃（不执行 SNAT ）。 `P143`

#### 通过 `Ingress` 暴露服务 `P143`

**为什么需要 `Ingress`** `P144`

每个 `LoadBalancer` 服务都需要自己的负载均衡器，以及独有的公有 IP 地址，而 `Ingress` 只需要一个公网 IP 就能为许多服务提供访问。当客户端向 `Ingress` 发送 HTTP 请求时， `Ingress` 会根据请求的主机名和路径决定请求转发到的服务。 `P144`

![图 5.9 通过一个 Ingress 暴露多个服务](img/chapter05/图%205.9%20通过一个%20Ingress%20暴露多个服务.png)

`Ingress` 在网络栈 (HTTP) 的应用层操作，并且可以提供一些服务不能实现的功能，例如基于 cookie 的会话亲和性 (session affinity) 等功能。 `P144`

**Ingress 控制器是必不可少的** `P144`

只有 `Ingress` 控制器在集群中运行， `Ingress` 资源才能正常工作。 `P144`

**在 minikube 上启动 `Ingress` 的扩展功能** `P145`

`minikube addons list`: 可以列出所有的插件及其启用状态

`minikube addons enable ingress`: 启用 `ingress` 插件

`kubectl get pods -n kube-system`: 查看 `kube-system` 命名空间下的 pod ，可以发现 `Ingress` 控制器 pod

##### 创建 `Ingress` 资源 `P145`

可以使用如下描述文件 `kubia-ingress.yaml` 创建一个 `Ingress` 资源。

```yaml
# 遵循 extensions/v1beta1 版本的 Kubernetes API
apiVersion: extensions/v1beta1
# 资源类型为 Ingress
kind: Ingress
metadata:
  # Ingress 的名称
  name: kubia
spec:
  # Ingress 的规则列表
  rules:
    # 第一条规则匹配的域名为 kubia.example.com
    - host: kubia.example.com
      # 匹配 http
      http:
        # 匹配的路径列表
        paths:
          # 第一条路径为 /
          - path: /
            # 该路径将被转发到的后端服务
            backend:
              # 将被转发到 kubia-nodeport 服务
              serviceName: kubia-nodeport
              # 对应服务的端口为 80
              servicePort: 80
```

minikube 下创建 `Ingress` 资源时报错了，提示超时。后来找到一种解决方案：使用 `kubectl edit ValidatingWebhookConfiguration/ingress-nginx-admission` 进行编辑，找到 `failurePolicy: Fail` 这行，并将 `Fail` 改为 `Ignore` ，然后就能成功创建 `Ingress` 资源了，等一段时间后就可以看见其分配了一个 IP 地址 (`192.168.64.70`) 。

为了能将指定的域名 `kubia.example.com` 指向分配的 IP 地址 (`192.168.64.70`)，可以使用 [SwitchHosts](https://github.com/oldj/SwitchHosts) 这个软件进行快速切换。

此时我们在主机上就可以通过 `curl kubia.example.com` 访问 `kubia-nodeport` 服务了。

**了解 `Ingress` 的工作原理** `P147`

1. 客户端对 `kubia.example.com` 执行 DNS 查找，本地操作系统返回了 `Ingress` 控制器的 IP
2. 客户端向 `Ingress` 控制器发送 HTTP 请求，并在 Host 头中指定 `kubia.example.com`
3. 控制器从头部确定客户端尝试访问哪个服务，通过与该服务关联的 `Endpoint` 对象查看 pod IP ，并将客户端的请求转发给其中一个 pod

![图 5.10 通过 Ingress 访问 pod](img/chapter05/图%205.10%20通过%20Ingress%20访问%20pod.png)

`Ingress` 控制器不会将请求转发给服务，只用它来选择一个 pod 。大多数控制器都是这样工作的。 `P147`

##### 通过相同的 `Ingress` 暴露多个服务 `P147`

`Ingerss` 的 `rules` 和 `paths` 都是数组，所以它们可以包含多个条目，因此一个 `Ingress` 可以将多个域名和路径映射到多个服务。 `P147`

##### 配置 `Ingress` 处理 TLS 传输 `P149`

**为 `Ingress` 创建 TLS 认证** `P149`

当客户端创建到 `Ingress` 控制器到 TLS 连接时，控制器将终止 TLS 连接。客户端和控制器之间到通信是加密的，而控制器和后端 pod 之间的通信则未加密。运行在 pod 上的应用程序不需要支持 TLS 。 `P149`

为了让 `Ingress` 控制器负责处理与 TLS 相关的所有内容，需要将证书和私钥附加到 `Ingress` 。这两个必须资源存储在称为 `Secret` 的 Kubernetes 资源中（将在第 7 章中详细介绍 `Secret`），然后在 `Ingress` 的描述文件中引用它。 `P149`

`openssl genrsa -out tls.key 2048`: 创建私钥

`openssl req -new -x509 -key tls.key -out tls.cert -days 365 -subj /CN=kubia.example.com`: 创建证书

`kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key`: 创建 `Secret` 资源

然后我们就可以改写 `kubia-ingress.yaml` 得到 `kubia-ingress-tls.yaml` 描述文件：

```yaml
...
spec:
  # 配置 TLS
  tls:
    # 第一条配置的域名列表
    - hosts:
      - kubia.example.com
      # 这些域名使用 tls-secret 获得私钥和证书
      secretName: tls-secret
  ...
```

然后我们就可以使用 `curl -k -v https://kubia.example.com` 通过 HTTPS 访问服务了。（ minikube 下未进行上述操作前也可以访问，不过可以发现是 `Ingress` 控制器使用了假证书） `P150`

#### pod 就绪后发出信号 `P150`

与存活探测器（[04. 副本机制和其他控制器：部署托管的 pod](04.%20副本机制和其他控制器：部署托管的%20pod.md) 中介绍过）类似， Kubernetes 还允许为容器定义就绪探测器。就绪探测器会定期调用，并确保特定的 pod 是否接收客户端请求。当容器的就绪探测器返回成功时，表示容器已准备好接收请求。 `P151`

**就绪探测器的类型**： `P151`

- `Exec` 探测器：在容器内执行任意命令，并检查命令的退出状态码。如果状态码是 0 ，则探测成功，认为容器已经就绪，所有其他状态码都被认为失败
- `HTTP GET` 探测器：对容器的 IP 地址（指定的端口和路径）执行 `HTTP GET` 请求。如果探测器收到响应，并且响应状态码不代表错误（状态码为 2xx 或 3xx ），则认为探测成功，认为容器已经就绪。如果服务器返回错误响应状态码或者没有响应，那么探测就被认为是失败的
- `TCP Socket`探测器：尝试与容器指定端口建立 TCP 连接。如果连接成功建立，则探测成功，认为容器已经就绪

**了解就绪探测器的操作** `P151`

启动容器时，可以为 Kubernetes 配置一个等待时间，经过等待时间后才可以执行第一次准备就绪检查。之后，它会周期性地调用探测器，并根据就绪探测器的结果采取行动。如果某个 pod 报告它尚未准备就绪，那么就会从服务中删除该 pod ；如果这个 pod 再次准备就绪，那么就会将给 pod 重新添加到服务中。 `P151`

![图 5.11 就绪探测失败的 pod 从服务的 endpoint 中移除](img/chapter05/图%205.11%20就绪探测失败的%20pod%20从服务的%20endpoint%20中移除.png)

**就绪探测器和存活探测器的区别** `P151`

存活探测器通过杀死异常的容器并用新的正常容器替代它们来保持 pod 正常工作，而就绪探测器确保只有准备好处理请求的 pod 才可以接收请求，并不会终止或重新启动容器。 `P151`

**就绪探测器的重要性**：确保客户端只与正常的 pod 交互，并且永远不会知道系统存在的问题。 `P152`

##### 了解就绪探测器的实际作用 `P154`

**务必定义就绪探测器** `P155`

应该始终定义一个就绪探测器，即使它只是向基准 URL 发送 HTTP 请求一样简单。如果没有将就绪探测器添加到 pod 中，那么它们启动后几乎立即成为服务端点。 `P155`

**不要将停止 pod 的逻辑纳入到就绪探测器中** `P155`

当一个容器关闭时，运行在其中的应用程序通常会在收到终止信号后立即停止接收连接。但在启动关机程序后，没有必要让就绪探测器返回失败以达到从所有服务中移除 pod 目的，因为在该容器删除后， Kubernetes 就会自动从所有服务中移除该容器。 `P155`

#### 使用 headless 服务来发现独立的 pod `P155`

要让客户端连接到所有 pod ，需要找出每个 pod 的 IP 。 Kubernetes 允许客户通过 DNS 查找发现 pod IP 。通常，当执行服务的 DNS 查找时， DNS 服务器会返回单个 IP —— 服务的集群 IP 。但是，如果告诉 Kubernetes ，不需要为服务提供集群 IP （通过在服务 `spec` 中将 `clusterIP` 字段设置为 `None` 来完成此操作），则 DNS 服务器将会返回 pod IP 而不是单个服务 IP 。 `P155`

DNS 服务器不会返回单个 DNS A 记录，而是会为该服务返回多个 A 记录，每个记录指向当时支持该服务的单个 pod 的 IP 。客户端因此可以做一个简单的 DNS A 记录查询并获取属于该服务的所有 pod 的 IP 。客户端可以使用该信息连接到其中的一个、多个或全部。 `P155`

##### 创建 headless 服务 `P156`

可以使用如下描述文件 `kubia-svc-headless.yaml` 创建一个 headless 的 `Service` 资源。

```yaml
# 遵循 v1 版本的 Kubernetes API
apiVersion: v1
# 资源类型为 Service
kind: Service
metadata:
  # Service 的名称
  name: kubia-headless
spec:
  # 该服务的集群 IP 为 None ，使其变为 headless 的
  clusterIP: None
  # 该服务可用的端口
  ports:
    # 第一个可用端口的名字
    - name: http
      # 可用端口为 80
      port: 80
      # 服务将连接转发到容器的 8080 端口
      targetPort: 8080
    # 第二个可用端口的名字
    - name: https
      # 可用端口为 443
      port: 443
      # 服务将连接转发到容器的 8443 端口
      targetPort: 8443
  # 具有 app=kubia 标签的 pod 都属于该服务
  selector:
    app: kubia
```

##### 通过 DNS 发现 pod `P156`

kubia 容器镜像不包含 `nslookup` 二进制文件，所以需要用一个新的容器镜像来执行相应的命令。 `P156`

`kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1 --command -- sleep infinity`: 创建一个可以执行 `nslookup` 命令的 pod

`kubectl exec dnsutils nslookup kubia-headless`: 在 `dnsutils` pod 内执行 `nslookup kubia-headless` 命令，可以发现 DNS 服务器为 `kubia-headless.default.svc.cluster.local` FQDN 返回了多个 IP ，且它们都是 pod 的 IP ，可以通过 `kubectl get pods -o wide` 进行确认

`kubectl exec dnsutils nslookup kubia`: 在 `dnsutils` pod 内执行 `nslookup kubia` 命令，可以发现 DNS 服务器为 `kubia.default.svc.cluster.local` FQDN 返回了一个 IP ，该 IP 是服务的集群 IP

尽管 headless 服务看起来可能与常规服务不同，但是在客户的视角上它们并无不同。对于 headless 服务，由于 DNS 返回了 pod 的 IP ，客户端直接连接到该 pod ，而不是通过服务代理（注意这里是直接访问的 pod ，所以对应的端口要改成 pod 的端口）。 `P157`

**注意**： headless 服务仍然提供跨 pod 的负载均衡，但是通过 DNS 轮循机制不是通过服务代理 `P157`

##### 发现所有的 pod —— 包括未就绪的 pod `P157`

可以通过在 `Service.metadata.annotations` 下面增加一条 `service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"` 告诉 Kubernetes 无论 pod 的准备状态如何，希望将所有 pod 添加到服务中 `P158`

#### 排除服务故障 `P158`

如果无法通过服务访问 pod ，应根据下面的列表进行排查： `P158`
- 确保从集群内连接到服务的集群 IP
- 不要通过 `ping` 服务 IP 来判断服务是否可访问（服务的集群 IP 是虚拟 IP ，是无法 ping 通的）
- 如果已经定义了就绪探测器，请确保它返回成功；否则该 pod 不会成为服务的一部分
- 要确认某个容器是服务的一部分，请使用 `kubectl get endpoints` 来检查相应的端点对象
- 如果尝试通过 FQDN 或其中一部分来访问服务，但并不起作用，请查看是否可以使用其集群 IP 而不是 FQDN 来访问服务
- 检查是否连接到服务公开的端口，而不是目标端口
- 尝试直接连接到 pod IP 以确认 pod 正在接收正确端口上的连接
- 如果甚至无法通过 pod 的 IP 访问应用，请确保应用不是仅绑定到 `localhost (127.0.0.1)`
