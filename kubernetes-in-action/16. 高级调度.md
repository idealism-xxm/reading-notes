## 使用污点和容忍度阻止节点调度到特定节点 `P463`

节点污点和 Pod 对于污点的容忍度被可被用于限制哪些 Pod 可以被调度到某一个节点。只有当一个 Pod 容忍某个节点的污点时，这个 Pod 才能被调度到该节点。 `P463`

- 节点的选择器和节点亲和性规则：通过明确的在 Pod 中添加的信息，来决定一个 Pod 是否可以被调度到某些节点上
- 污点和容忍度：在不修改已有 Pod 信息的前提下，通过在节点上添加污点信息，来拒绝 Pod 在某些节点上的部署

### 介绍污点和容忍度 `P464`

默认情况下，通过 `kubeadm` 工具创建的一个多节点集群中的主节点需要设置污点，这样才能保证只有控制面板 Pod 才能部署在主节点上。 `P464`

![图 16.1 只有容忍了节点污点的 Pod 才能被调度到节点上](img/chapter16/图%2016.1%20只有容忍了节点污点的%20Pod%20才能被调度到节点上.png)

#### 显示节点的污点信息 `P464`

`kubectl describe node minikube | grep -A 10 Taints:`: 获取节点 minikube 的描述信息，并仅截取污点信息开始的 10 行内容 `P464`

污点的格式一般如下： `<key>=<value>:<effect>` ，其中 `<value>` 可以为空。 `P464`

#### 显示 Pod 的污点容忍度 `P465`

`kubectl describe node minikube | grep -A 10 Taints:`: 获取 Pod 的描述信息，并仅截取污点容忍度信息开始的 10 行内容 `P465`

```shell script
Tolerations:     op=Exists
                 CriticalAddonsOnly op=Exists
                 node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                 node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                 node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                 node.kubernetes.io/not-ready:NoExecute op=Exists
                 node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                 node.kubernetes.io/unreachable:NoExecute op=Exists
                 node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:          <none>
```

#### 了解污点的效果 `P466`

每一个污点都可以关联一个效果，效果包含了以下三种： `P466`

- `NoSchedule`: 如果 Pod 不能容忍这个污点，那么 Pod 不能被调度到包含这个污点的节点上
- `PreferNoSchedule`: 是 `NoSchedule` 的一个宽松版本，表示尽量阻止 Pod 被调度到这个节点上，但是如果没有其他节点可以调度， Pod 依然会被调度到这个节点
- `NoExecute`: 前面两个只在调度期间起作用，而 `NoExecute` 也会影响正在节点上运行着的 Pod 。如果一个节点上添加了 `NoExecute` 污点，那么不能容忍这个污点的 Pod 就会从该节点上去除

### 在节点上添加自定义污点 `P466`

`kubectl taint node minikube node-type=production:NoSchedule`: 在节点 minikube 上添加一个 `key` 为 `node-type` ， `value` 为 `production` ，效果为 `NoSchedule` 的污点

### 在 Pod 上添加污点容忍度 `P467`

可以在 Deployment 的描述文件中添加以下信息使得 Pod 能够容忍上述污点： `P467`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: prod
spec:
  ...
  template:
    spec:
      ...
      tolerations:
        - key: node-type
          operator: Equal
          value: production
          effect: NoSchedule
      ...
```

### 了解污点和污点容忍度的使用场景 `P467`

节点可以拥有多个污点信息，而 Pod 也可以拥有多个污点容忍度。污点可以只有一个 `key` 和一个效果，而不必设置 `value` 。污点容忍度可以通过设置 `Equal` 操作符（默认情况下的操作符）来指定匹配的 `value` ，或者也可以通过设置 `Exists` 操作符来匹配污点的 `key` 。 `P467`

#### 在调度时使用污点和容忍度 `P468`

污点可以用来组织新 Pod 的调度（使用 `NoSchedule` 效果），或者定义非优先调度的节点（使用 `PreferNoSchedule` 效果），甚至是将已有的 Pod 从当前节点剔除。 `P468`

可以将一个集群分成多个部分，只允许开发团队将 Pod 调度到特定的节点上。当部分节点提供了某种特殊硬件，并且只有部分 Pod 需要使用到这种硬件的时候，也可以通过设置污点和容忍度来实现。 `P468`

#### 配置节点失效之后的 Pod 重新调度最长等待时间 `P468`

`Pod.spec.tolerations.tolerationSeconds` 可以配置当节点拥有某个效果为 `NoExecute` 的污点后，该 Pod 最长可以忍受的时间，如果超过该时间后节点还有该污点，那么 Pod 就会被从当前节点剔除。默认情况下，该字段未设置，表示可以永远忍受该污点；负数和零表示该 Pod 会被立刻剔除。 `P468`

## 使用节点亲和性将 Pod 调度到特定节点上 `P469`

### 简介

污点可以用来让 Pod 远离特定的节点，亲和性允许 Kubernetes 将 Pod 只调度到节点的某些子集上。 `P469`

#### 对比节点亲和性和节点选择器 `P469`

- 节点选择器：通过 `Pod.spec.nodeSelector` 字段进行设置，节点必须包含所有指定的标签才能成为 Pod 调度的目标节点
- 节点亲和性：通过 `Pod.spec.affinity.nodeAffinity` 字段进行设置，亲和性规则可以允许指定硬性限制或者偏好。如果指定一种偏好， Kubernetes 对于该 Pod 会更倾向于调度到某些节点上，如果没法实现，该 Pod 则会被调度到某个满足硬性限制的节点上

### 指定节点亲和性硬性限制 `P469`

[03. pod: 运行于 Kubernetes 中的容器](03.%20pod%3A%20运行于%20Kubernetes%20中的容器.md) 中使用了节点选择器让 Pod 调度到特定节点，相关片段如下： `P470`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
  nodeSelector:
    gpu: "true"
  ...
```

转换成等价的节点亲和性规则，相关片段则如下所示：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExperessions:
              - key: gpu
                operator: In
                values:
                  - "true"
```

#### 节点亲和性属性名的意义 `P471`

`requiredDuringSchedulingIgnoredDuringExecution` 可以分成两个部分，然后分别看其含义： `P471`

- `requiredDuringScheduling...`: 表明了该字段下定义的规则，为了让 Pod 能调度到该节点上，明确指出了该节点必须包含的标签
-  `...IgnoredDuringExecution`: 表明了该字段下定义的规则，不会影响已经在节点上运行的 Pod

#### 了解 `nodeSelectorTerms` `P471`

`nodeSelectorTerms` 和 `matchExpressions` 定义了节点的标签必须满足哪一种表达式，才能满足 Pod 调度的条件 `P471`

![图 16.2 Pod 的亲和性制定了节点必须包含满足调度条件的标签](img/chapter16/图%2016.2%20Pod%20的亲和性制定了节点必须包含满足调度条件的标签.png)

### 调度 Pod 时优先考虑某些节点 `P472`

节点亲和性可以指定调度器调度某一个 Pod 时优先考虑哪些节点，这个功能是通过 `preferredDuringSchedulingIgnoredDuringExecution` 字段实现的。 `P472`

#### 指定节点亲和性偏好 `P472`

使用如下亲和性设置可以让 Pod 优先被调度到 zone1 中的 dedicated 节点： `P473`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        # 优先调度到 zone1 ，它所占的权重最高
        - weight: 80
          preference:
            matchExpressions:
              - key: availability-zone
                operator: In
                values:
                  - zone1
        # 同时优先调度到 zone1 ，它的权重是前者的 1/4
        - weight: 20
          preference:
            matchExpressions:
              - key: share-type
                operator: In
                values:
                  - dedicated
```

![图 16.3 基于 Pod 节点亲和性优先级对节点排序](img/chapter16/图%2016.3%20基于%20Pod%20节点亲和性优先级对节点排序.png)

如果使用如上节点亲和性偏好配置一个 5 副本的 Deployment ，那么可能并非所有的节点都会在优先级最高的节点上，因为除了节点亲和性的优先级函数，调度器还会使用其他的优先级函数来决定 Pod 被调度到哪。其中之一就是 `SelectorSpreadPriority` 函数，这个函数为了更好的容灾，会尽量将属于同一个 ReplicaSet 或者 Service 的 Pod 分散部署在不同的节点上。 `P475`

## 使用 Pod 间亲和与反亲和对 Pod 进行协同部署 `P475`

### 使用 Pod 间亲和将多个 Pod 部署在同一个节点上 `P475`

假设我们需要部署 1 个后端 Pod 和 5 个前端 Pod ，并且希望这些前端 Pod 将被部署在后端 Pod 所在的同一个节点上。 `P475`

我们可以先通过 Deployment 部署一个带标签 `app=backend` 的后端 Pod ，然后我们可以在前端 Deployment 的 Pod 模版中指定 Pod 间亲和，相关代码片段如下： `P476`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: frontend
spec:
  ...
  template:
    spec:
      affinity:
        # 定义 Pod 间亲和
        podAffinity:
          # 定义该 Pod 间亲和的一个硬性限制
          requiredDuringSchedulingIgnoredDuringExecution:
            # 本次部署的 Pod 必须被调度到包含 app=backend 标签的 Pod 所在的节点（通过 topologyKey 指定）上
            - topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app: backend
      ...
```

![图 16.4 Pod 间亲和限制 Pod 必须被调度到包含指定标签的 Pod 所在节点上](img/chapter16/图%2016.4%20Pod%20间亲和限制%20Pod%20必须被调度到包含指定标签的%20Pod%20所在节点上.png)

**提示**：除了使用简单的 `matchLabels` 字段，也可使用表达能力更强的 `matchExpressions` 字段。 `P477`

#### 了解调度器如何使用 Pod 间亲和 `P477`

如果以上 Pod 都部署完，删掉后端 Pod ，那么调度器会将新的后端 Pod 调度到相同节点，即使只有前端 Pod 设置了 Pod 间亲和，而后端 Pod 没有设置任何 Pod 间亲和。 `P477`

如果增加调度器的日志级别，并观察调度器的日志，可以确定调度器会考虑其他 Pod 的 Pod 间亲和： `P477`

```shell script
... Attempting to schedule pod: default/backend-257820-qhqj6
... ...
... backend-qhqj6 -> node2.k8s: Taint Toleration Priority, Score: (10)
... backend-qhqj6 -> node1.k8s: Taint Toleration Priority, Score: (10)
... backend-qhqj6 -> node2.k8s: InterPodAffinityPriority, Score: (10)
... backend-qhqj6 -> node1.k8s: InterPodAffinityPriority, Score: (0)
... backend-qhqj6 -> node2.k8s: SelectorSpreadPriority, Score: (10)
... backend-qhqj6 -> node1.k8s: SelectorSpreadPriority, Score: (10)
... backend-qhqj6 -> node2.k8s: NodeAffinityPriority, Score: (0)
... backend-qhqj6 -> node1.k8s: NodeAffinityPriority, Score: (0)
... Host node2.k8s => Score 100030
... Host node1.k8s => Score 100022
... Attempting to bind backend-257820-qhqj6 to node2.k8s 
```

可以发现 Pod 间亲和那一项的分数，原本的节点获得了更高的分数。 `P478`

**注意**：Pod 间亲和与反亲和需要大量的处理，这可能会显著减慢大规模集群中的调度，不建议在超过数百个节点的集群中使用它们。

### 使用 Pod 间亲和按其他方式调度 `P478`

在前面的例子中，我们通过指定了 `topologyKey` 字段的值为 `kubernetes.io/hostname` 让前后端 Pod 在同一个节点上，我们也希望 Pod 间亲和能按照其它方式调度。 `P478`

- 在同一个可用性区域中协同部署 Pod ：指定 `topologyKey` 字段的值为 `failure-domain.beta.kubernetes.io/zone`
- 在同一个地域中协同部署 Pod ：指定 `topologyKey` 字段的值为 `failure-domain.beta.kubernetes.io/region`

#### 了解 `topologyKey` 是如何工作的 `P478`

`topologyKey` 的工作方式很简单，其值是一个节点的标签键，可以任意设置自定义的键，不过存在一些限制，可查看 [Inter-pod affinity and anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity) 获取更详细信息。 `P479`

- 当调度器决定 Pod 调度到哪里时，它首先检查 Pod 的 `podAffinity` 配置，找出那些符合标签选择器的 Pod 
- 接着调度器查询这些 Pod 运行在哪些节点上，特别是那些标签匹配 `podAffinity` 配置了 `topologyKey` 指定的标签键的节点
- 然后调度器会仅 (`required`) / 优先 (`preferred`) 选择标签与这些节点的标签匹配的那些节点进行调度

**注意**：在调度时，默认情况下，标签选择器只有匹配同一命名空间中的 Pod 。但是，可以通过在 `labelSelector` 同一级添加 `namespaces` 字段，实现从其他的命名空间选择 Pod 的功能。 `P479`

![图 16.5 podAffinity 中的 topologyKey 决定了 Pod 被调度的范围](img/chapter16/图%2016.5%20podAffinity%20中的%20topologyKey%20决定了%20Pod%20被调度的范围.png)

在上图中，标签选择器匹配了运行在节点 Node12 的后端 Pod ，该节点 `rack` 标签的值等于 `rack2` 。所以，当调度 1 个前端 Pod 时，调度器只会在包含标签 `rack=rack2` 的节点中选择。 `P479`

### Pod 间亲和偏好 `P479`

`podAffinity` 和 `nodeAffinity` 一样，既可以设置硬性限制让调度器只能将 Pod 调度到哪些节点上；也可以设置偏好让调度器优先将 Pod 调度到哪些节点上，不满足要求时调度到其他节点。 `P480`

偏好配置和节点的偏好类似，相关代码片段如下： `P480`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: frontend
spec:
  ...
  template:
    spec:
      affinity:
        # 定义 Pod 间亲和
        podAffinity:
          # 定义该 Pod 间亲和的一个偏好
          preferredDuringSchedulingIgnoredDuringExecution:
            # 本次部署的 Pod 优先被调度到包含 app=backend 标签的 Pod 所在的节点（通过 topologyKey 指定）上，该优先级的权重为 80
            - weight: 80
              podAffinityTerm:
                - topologyKey: kubernetes.io/hostname
                  labelSelector:
                    matchLabels:
                      app: backend
      ...
```

![图 16.6 Pod 间亲和偏好让调度器优先考虑某些节点](img/chapter16/图%2016.6%20Pod%20间亲和偏好让调度器优先考虑某些节点.png)

### 利用 Pod 间反亲和分开调度 Pod `P481`

Pod 间反亲和的表示方式和 Pod 间亲和表示方式一样，将 `podAffinity` 字段换成 `podAntiAffinity` 即可，这将导致调度器不会 (`required`) / 尽量避免 (`preferred`) 选择那些有包含 `podAntiAffinity` 匹配标签的 Pod 所在的节点。 `P481`

![图 16.7 Pod 间反亲和使得 Pod 远离包含某些标签 Pod 所在的节点](img/chapter16/图%2016.7%20Pod%20间反亲和使得%20Pod%20远离包含某些标签%20Pod%20所在的节点.png)

Pod 间反亲和可以让 Pod 分散部署，不同限制的场景如下：

- 硬性限制：同类 Pod 运行在同一个节点/可用区域/地域会出现问题时用
- 偏好：同类 Pod 运行在同一个节点/可用区域/地域不是大问题
