## 了解架构 `P315`

### Kubernetes 的组件 `P315`

#### 控制面板的组件 `P316`

控制面板负责控制并使得整个集群正常运转，包含如下组件： `P316`

- etcd 分布式持久化存储
- API 服务器
- 调度器
- 控制器管理器

这些组件用来存储、管理集群状态，但它们不是运行应用的容器。 `P316`

#### 工作节点上运行的组件 `P316`

运行容器的任务依赖于每个工作节点上运行的组件： `P316`

- Kubelet
- Kubelet 服务代理 (kube-proxy)
- 容器运行时 (Docker, rkt 或其他)

#### 附加组件 `P316`

除了控制面板和运行在节点上的组件，还要有几个附加组件才能提供所有之前讨论的功能。 `P316`

- Kubernetes DNS 服务器
- Dashboard
- Ingress 控制器
- Heapster （容器集群监控）
- 容器网络接口插件

### Kubernetes 组件的分布式特性 `P316`

![图 11.1 Kubernetes 控制面板以及工作节点的组件之间的依赖关系](img/chapter11/图%2011.1%20Kubernetes%20控制面板以及工作节点的组件之间的依赖关系.png)

若要启用 Kubernetes 提供的所有特性，需要运行所有组件。但是有几个组件无须其他组件，单独运行也能提供非常有用的功能。 `P316`

#### 检查控制面板组件的状态 `P317`

API 服务器对外暴露了一个名为 ComponentStatus 的 API 资源，用来显示控制面板组件的健康状态。 `P317`

`kubectl get componentstatuses`: 列出各个组件以及它们的状态。 `P317`

```shell script
NAME                 STATUS    MESSAGE             ERROR
controller-manager   Healthy   ok                  
scheduler            Healthy   ok                  
etcd-0               Healthy   {"health":"true"} 
```

#### 组件间如何通信 `P317`

Kubernetes 系统组件间只能通过 API 服务器通信，它们之间不会直接通信。 API 服务器是和 etcd 通信的唯一组件。其他组件不会和 etcd 通信，而是通过 API 服务器来修改集群状态。 `P317`

API 服务器和其他组件的连接基本都是组件发起的，如图 11.1 所示。但是当使用 `kubectl` 获取日志、使用 `kubectl attach` 连接到一个运行中的容器或者运行 `kubectl port-forward` 命令时， API 服务器会向 Kubelet 发起连接。 `P317`

**注意**： `kubectl attach` 命令和 `kubectl exec` 命令类似，区别是：前者会附属到容器中运行着的主进程上，而后者是重新运行一个进程。 `P317`

#### 单组件运行多实例 `P317`

工作节点上的组件都需要运行在同一个节点上，控制面板的组件可以被简单地分隔在多台服务器上。为了保证高可用性，控制面板的每个组件都可以有多个实例。 etcd 和 API 服务器的多个实例可以同时并行工作。但是调度器和控制器管理器同时只能有一个实例起作用，其他实例处于待命模式。 `P317`

#### 组件是如何运行的 `P318`

控制面板的组件以及 kube-proxy 可以直接部署在系统上或者作为 Pod 来运行。 `P318`

Kubelet 是唯一一直作为常规系统组件来运行的组件，它把其他组件作为 Pod 来运行。为了将控制面板作为 Pod 来运行， Kubelet 被部署在 master 上。 `P318`

`kubectl get pods -o custom-columns=POD:metadata.name,Node:spec.nodeName --sort-by spec.nodeName -n kube-system`: kube-system 命名空间下的 Pod 及其所在的 Node `P318`

```shell script
POD                                Node
# etcd 、 API 服务器、调度器、控制器管理器和 DNS 服务器运行在 master 上 
coredns-66bff467f8-47pcr           minikube
kube-scheduler-minikube            minikube
coredns-66bff467f8-8lxfk           minikube
kube-proxy-58n7k                   minikube
etcd-minikube                      minikube
kube-controller-manager-minikube   minikube
kube-apiserver-minikube            minikube
kindnet-qxmhj                      minikube
storage-provisioner                minikube
# 三个节点上均运行 Kube Proxy Pod 和一个 kindnet 网络 Pod
kindnet-bsr4q                      minikube-m02
coredns-66bff467f8-zr52f           minikube-m02
kube-proxy-bf98k                   minikube-m02
kindnet-tpq7l                      minikube-m03
kube-proxy-kdssx                   minikube-m03
coredns-66bff467f8-752fr           minikube-m03
kube-proxy-ls4d5                   minikube-m04
kindnet-5p29k                      minikube-m04
```

#### Kubernetes 如何使用 etcd `P318`

Kubernetes 中创建的所有对象 —— Pod 、 Deployment 、服务和私密凭证等，需要以持久化方式存储到某个地方，这样它们的 manifest 在 API 服务器重启和失败的时候才不会丢失。 etcd 是一个响应快、分布式、一致的键值存储。 `P318`

唯一能直接和 etcd 通信的是 Kubernetes 的 API 服务器。所有其他组件通过 API 服务器间接地读取、写入数据到 etcd 。 etcd 是 Kubernetes 存储集群状态和元数据的唯一地方。 `P319`

好处： `P319`

- 增强乐观锁系统、验证系统的健壮性
- 通过把实际存储机制从其他组件抽离，未来替换起来也更容易

**乐观并发控制**：所有 Kubernetes 包含一个 `metadata.resourceVersion` 字段，当更新对象时，客户端需要返回该值到 API 服务器。如果版本值与 etcd 中存储的不匹配， API 服务器就会拒绝该更新。 `P319`

#### 资源如何存储在 etcd 中 `P319`

| 版本 | v2 | v3 | 
| --- | --- | --- |
| 存储方式 | 一个 key 要么是一个目录，包含其他 key；要么是一个常规 key ，对应一个 value | 不支持目录，但 key 可以包含 `/` ，所以仍然可以被组织为目录 |
| 查看一系列 key | `etcdctl ls /registry` | `etcdctl get /registry --prefix=true` |
| 查看单个 key | `etcdctl get /registry/pods/default/kubia-0` | 与 v2 相同 |

查看单个 key 时，可以发现 value 就是一个 JSON 格式的 Pod 定义。 `P320`

#### 确保存储对象的一致性和可验证性 `P320`

Kubernetes 要求所有控制面板组件只能通过 API 服务器操作存储模块。这样保证了更新集群状态总是一致的，因为 API 服务器实现了乐观锁错误机制，同时确保写入存储的数据总是有效的，只有授权的客户端才能更改数据。 `P321`

#### 确保 etcd 集群一致性 `P321`

多个 etcd 实例使用 RAFT 一致性算法来保证一致性，确保在任何时间点，每个节点的状态要么是大部分节点的当前状态，要么是之前确认过的状态。 `P321`

连接到 etcd 集群不同节点的客户端，得到的要么是当前的实际状态，要么是之前的状态（ Kubernetes 中， etcd 的唯一客户端是 API 服务器，但可能有多个实例）。 `P321`

一致性算法要求集群大部分（法定数量）节点参与才能进行到下一个状态。如果集群分裂为两个不互联的节点组，两个组的状态不可能不一致，因为要从之前状态变化到新状态，需要有过半的节点参与状态变更。如果一个组包含了大部分节点，那么另一组只有少部分节点。第一个组就可以更改集群状态，第二个组则不可以。当两个组重新恢复连接，第二个组的节点会更新为第一个组的节点状态。 `P321`

![图 11.2 在脑裂场景中，只有拥有大部分（法定数量）节点的组会接受状态变更](img/chapter11/图%2011.2%20在脑裂场景中，只有拥有大部分（法定数量）节点的组会接受状态变更.png)

通常对于大集群， etcd 集群有 5 个或 7 个节点就足够了。可以允许 2~3 个节点宕机，这对大多数场景来说足够了。 `P322`

### API 服务器做了什么 `P322`

- 提供一种一致的方式将对象存储到 etcd
- 校验对象，让客户端（ `kubectl` 等）无法存储非法对象
- 处理乐观锁

![图 11.3 API 服务器的操作](img/chapter11/图%2011.3%20API%20服务器的操作.png)

#### 通过认证插件认证客户端 `P323`

API 服务器需要认证发送请求的客户端。这是通过配置在 API 服务器上的一个或多个认证插件来实现的。 API 服务器会轮流调用这些插件，直到有一个能确认是谁发送了该请求。插件抽取客户端的用户名、用户 ID 和归属组。这些数据在下一阶段，授权的时候会用到。 `P323`

#### 通过授权插件授权客户端 `P323`

API 服务器还可以配置使用一个或多个授权插件。它们的作用是决定认证的用户是否可以对请求资源执行请求操作。一旦插件确认了用户可以执行该操作， API 服务器会继续下一步操作。 `P323`

#### 通过准入控制插件验证 AND/OR 修改资源请求 `P323`

如果请求尝试创建、修改或者删除一个资源，请求需要经过准入控制插件的验证（尝试读取则不需要）。服务器会配置多个准入控制插件，这些插件会因为各种原因修改资源，可能会初始化资源定义中漏配的字段为默认值甚至重写它们，甚至会去修改并不在请求中的相关资源，同时也会因为某些原因拒绝一个请求。资源需要经过有准入控制插件的验证。 `P323`

准入控制插件包括： `P323`

- `AlwaysPullImages`: 重写 Pod 的 `imagePullPolicy` 为 `Always` ，强制每次部署 Pod 时拉取镜像
- `ServiceAccount`: 未明确定义服务账户时使用默认账户
- `NamespaceLifecycle`: 防止在命名空间中创建正在被删除的 Pod ，或在不存在的命名空间中创建 Pod
- `ResourceQuota`: 保证特定命名空间中的 Pod 只能使用该命名空间分配数量的资源，如 CPU 和内存

更多的准入控制插件可以在 [Using Admission Controllers](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do) 中找到。 `P323`

#### 验证资源以及持久化存储 `P323`

请求通过了所有的准入控制插件后， API 服务器会验证存储到 etcd 的对象，然后返回一个响应给客户端。 `P323`

### API 服务器如何通知客户端资源变更 `P324`

当 API 服务器做完前面所说的工作后，它会启动控制器以及其他一些组件来监控已部署资源的变更。控制面板可以请求订阅资源被创建、修改或删除的通知。这使得组件可以在集群元数据变化时执行任何需要做的任务。 `P324`

客户端通过创建到 API 服务器的 HTTP 连接来监听变更。通过此连接，客户端会接受到监听对象的一系列变更通知。每当更新对象，服务器把新版本对象发送至所有监听该对象的客户端。 `P324`

![图 11.4 更新对象时， API 服务器给所有监听者发送更新过的对象](img/chapter11/图%2011.4%20更新对象时，%20API%20服务器给所有监听者发送更新过的对象.png)

`kubectl get pods --watch`: 监听 Pod 列表，每当创建、修改、删除 Pod 时就会通知你，并打印出改变的 Pod 的最新状态 `P324`

### 了解调度器 `P325`

监听机制同样也适用于调度器。调度器利用 API 服务器的监听机制等待新创建的 Pod ，然后给每个新的、没有节点集的 Pod 分配节点。 `P324`

调度器不会命令选中的节点（或其上的 Kubelet ）去运行 Pod ，而是通过 API 服务器更新 Pod 的定义，然后 API 服务器通过监听机制通知 Kubelet 该 Pod 已经被调度过。当目标节点上的 Kubelet 发现该 Pod 被调度至本节点，它就会创建并运行 Pod 的容器。 `P325`

#### 默认的调度算法 `P325`

选择节点可以分解为两部分： `P325`

1. 过滤所有节点，找出能分配给 Pod 的可用节点列表
2. 对可用节点按照优先级排序，找出最优节点。如果多个节点都有最高的优先级分数，那么则循环分配，确保平均分配给 Pod

![图 11.5 调度器为 Pod 找到可用节点，然后选择最优节点](img/chapter11/图%2011.5%20调度器为%20Pod%20找到可用节点，然后选择最优节点.png)

#### 查找可用节点 `P325`

调度器会给每个节点下发一组配置好的预测函数，这些函数检查： `P325`

- 节点是否能满足 Pod 对硬件资源的请求（第 14 章中介绍）
- 节点是否耗尽资源（是否报告过内存/磁盘压力参数）
- Pod 是否要求被调度到指定节点（通过名字），是否是当前节点
- 节点是否有和 Pod 规格定义里的节点选择器一致的标签（如果定义了）
- 如果 Pod 要求绑定指定的主机端口，那么这个节点上的这个端口是否已经被占用
- 如果 Pod 要求有特定类型的卷，该节点是否能为此 Pod 加载此卷，或者说该节点上是否已经有 Pod 在使用该卷了
- Pod 是否能容忍节点的污点？（第 16 章中讲解）
- Pod 是否定义了节点、 Pod 的亲缘性以及非亲缘性规则？如果是，那么调度节点给该 Pod 是否违反规则？（第 14 章中介绍）

所有以上测试都必须通过，节点才有资格调度给 Pod 。 `P326`

#### 为 Pod 选择最佳节点 `P326`

选择最佳的节点与具体使用场景有关： `P326`

- 一个节点运行了 10 个 Pod ，另一个节点没有运行 Pod ，从节点压力上考虑，更倾向于使用第二个节点
- 两个节点都是云平台提供的，从成本上考虑，更倾向于使用第一个节点
- 一个 Pod 的多个副本更倾向于分散在不同的节点上，提高容错率

#### 使用多个调度器 `P327`

可以在集群中运行多个调度器，然后对每一个 Pod ，可以通过在 Pod 中设置 `Pod.spec.schedulerName` 属性指定调度器来调度特定的 Pod ，如果未设置，则会被默认调度器调度。 `P327`

### 介绍控制器管理器中运行的控制器 `P327`

API 服务器只做了存储资源到 etcd 和通知客户端有变更的工作，调度器则只是给 Pod 分配节点，控制器管理器的控制器确保系统真实状态朝 API 服务器定义的期望的状态收敛。 `P327`

一个控制器管理器进程会组合多个执行不同非冲突任务的控制器，这些控制器最终会被分配到不同的进程中。 `P327`

每个控制器做什么通过名字显而易见： `P327`

- ReplicaSet, DaemonSet 以及 Job 控制器
- Deployment 控制器
- StatefulSet 控制器
- Node 控制器
- Service 控制器
- Endpoints 控制器
- Namespace 控制器
- PersistentVolume 控制器
- 其他

资源描述了集群中应该运行什么，控制器就是活跃的 Kubernetes 组件，去做具体工作部署资源。 `P327`

#### 了解控制器做了什么以及如何做的 `P327`

控制器将实际状态调整为期望状态，然后将新的实际状态写入资源的 `status` 部分。控制器利用监听机制来订阅变更，但是由于使用监听机制并不保证控制器不会漏掉事件，所以仍然需要定期执行重列举操作来确保不会丢掉任何东西。 `P328`

控制器之间不会直接通信，它们甚至不知道其他控制器的存在，每个控制器都连接到 API 服务器，通过监听机制请求订阅该控制器负责的一系列资源的变更。 `P328`

#### ReplicaSet 、DeamonSet 以及 Job 控制器 `P329`

启动 ReplicaSet 资源的控制器叫做 ReplicaSet 控制器，它通过监听机制订阅可能影响期望的副本数或者符合条件 Pod 数量的变更事件，任何该类型的变化，将触发控制器重新检查期望的和实际的副本数量，然后作出响应操作。 `P329`

当运行的 Pod 实例太少时， ReplicaSet 会运行额外的实例，但它自己实际上不会去运行 Pod 。它会创建新的 Pod 清单，发布到 API 服务器，让调度器以及 Kubelet 来做调度工作并运行 Pod 。 `P329`

![图 11.6 ReplicaSet 管理器监听 API 对象变更](img/chapter11/图%2011.6%20ReplicaSet%20管理器监听%20API%20对象变更.png)

ReplicaSet 控制器通过 API 服务器操纵 Pod API 对象来完成其工作，所有控制器都是这样工作的。 `P329`

DaemonSet 和 Job 控制器比较相似，从它们各自资源集中定义的 Pod 模版创建 Pod 资源，其他操作与 ReplicaSet 控制器类似。 `P329`

#### Deployment 控制器 `P329`

Deployment 控制器负责使 Deployment 的实际状态与对应 Deployment API 对象的期望同步。 `P329`

#### StatefulSet 控制器 `P329`

其他控制器只管理 Pod ，而 StatefulSet 控制器会初始化并管理每个 Pod 实例的持久卷声明字段。 `P329`

#### Node 控制器 `P330`

Node 控制器管理 Node 资源，描述了集群工作节点。它使节点对象列表与集群中实际运行的机器列表保持同步，同时会监控每个节点的监控状态，删除不可达节点的 Pod 。 `P330`

Node 控制器不是唯一对 Node 对象做更改的组件。 Kubelet 也可以做更改，所以用户可以通过 REST API 调用更改 Node 对象。 `P330`

#### Service 控制器 `P330`

Service 控制器就是用来在服务被创建或删除时，从基础设施服务请求、释放对应类型的资源。 `P330`

#### Endpoints 控制器 `P330`

Endpoints 控制器作为活跃的组件，定期根据匹配标签选择器的 Pod 的 IP 和端口更新端点列表。 `P330`

Endpoints 控制器同时监听了 Service 和 Pod 。当 Service 被添加、修改，或者 Pod 被添加、修改、删除时，控制器会选中匹配 Service 的 Pod 选择器的 Pod ，将其 IP 和端口添加到 Endpoints 资源中。 Endpoints 对象是独立对象，所以会随着 Service 创建、删除。 `P330`

![图 11.7 Endpoints 控制器监听 Service 和 Pod 资源并管理 Endpoints](img/chapter11/图%2011.7%20Endpoints%20控制器监听%20Service%20和%20Pod%20资源并管理%20Endpoints.png)

#### Namespace 控制器 `P331`

当收到删除 Namespace 对象的通知时，控制器通过 API 服务器删除所有归属该命名空间的资源。 `P331`

#### PersistentVolume 控制器 `P331`

对于一个持久卷声明，控制器为声明查找最佳匹配项，通过选择匹配声明中的访问模式，并且声明的容量大于需求的容量的最小持久卷。当用户删除持久卷声明时，会解绑卷，然后根据卷的回收策略进行回收。 `P331`

#### 唤醒控制器 `P331`

所有控制器都是通过 API 服务器来操作 API 对象的，它们不会直接和 Kubelet 通信或者发送任何类型的指令，也不知道 Kubelet 的存在。控制器更新 API 服务器的一个资源后， Kubelet 和 Kubernetes  Service Proxy (也不知道控制器的存在) 会做它们的工作，例如：启动 Pod 容器、加载网络存储或者创建跨 Pod 的负载均衡。 `P331`


### Kubelet 做了什么 `P331`

控制面板处理了整个系统的一部分操作并且运行在主节点上， Kubelet 以及 Kubernetes Service Proxy 都运行在工作节点上。 `P331`

#### 了解 Kubelet 的工作内容 `P331`

Kubelet 就是负责所有运行在工作节点上内容的组件： `P331`
- 在 API 服务器中创建一个 Node 资源来注册该节点
- 持续监控 API 服务器是否把该节点分配给 Pod
- 启动 Pod 容器：告知配置好的容器运行时 (Docker, Rkt 或其他一些东西) 来从特定容器镜像运行容器
- 持续监控运行的容器，向 API 服务器报告它们的状态、事件和资源消耗
- 运行容器存活探测器，当探测器报错时会重启容器
- 当 Pod 从 API 服务器删除时，终止容器，并通知服务器 Pod 已经被终止了

#### 抛开 API 服务器运行静态 Pod `P332`

Kubelet 一般会和 API 服务器通信并从中获取 Pod 清单，但也能基于本地指定目录下的 Pod 清单运行 Pod ，所以可以将容器化版本的控制面板组件以 Pod 形式运行。 `P332`

![图 11.8 Kubelet 基于 API 服务器|本地文件目录中的 Pod 定义运行 Pod](img/chapter11/图%2011.8%20Kubelet%20基于%20API%20服务器%7C本地文件目录中的%20Pod%20定义运行%20Pod.png)

### Kubernetes Service Proxy 的作用 `P332`

kube-proxy 用于确保客户端可以通过 Kubernetes API 连接到你定义的服务。 kube-proxy 确保对服务 IP 和端口连接最终能达到支持服务的某个 Pod 处（或其他非 Pod 服务终端）。如果有多个 Pod 支撑一个服务，那么代理会发挥对 Pod 的负载均衡作用。 `P332`

kube-proxy 有两种代理模式： `P333`

- userspace 代理模式：利用实际的服务器集成接受连接，同时代理给 Pod 。为了拦截发往服务 IP 的连接，代理配置了 iptables 规则，重定向连接到代理服务器

![图 11.9 userspace 代理模式](img/chapter11/图%2011.9%20userspace%20代理模式.png)

- iptables 代理模式：仅通过 iptables 规则重定向数据包到一个随机选择的后端 Pod ，而不会传递到一个实际的代理服务器，性能更好

![图 11.10 iptables 代理模式](img/chapter11/图%2011.10%20iptables%20代理模式.png)

两种模式的区别： `P333`

- 数据包是否会传递给 kube-proxy ，是否必须在用户空间处理，或者数据包只会在内核空间处理，这对性能有巨大的影响
- userspace 代理模式以轮询模式对连接做负载均衡，而 iptables 代理模式随机选择一个 Pod

### 介绍 Kubernetes 插件 `P333`

Kubernetes 插件不是必需的，它们用于启用 Kubernetes 服务的 DNS 查询，通过单个外部 IP 地址暴露多个 HTTP 服务、 Kubernetes web Dashboard 等特性。 `P333`

#### 如何部署插件 `P334`

通过提交 YAML 描述文件到 API 服务器，这些组件会成为插件并作为 Pod 部署。 `P334`

#### DNS 服务器如何工作 `P334`

集群中所有的 Pod 默认配置使用集群内部 DNS 服务器，这使得 Pod 能够轻松地通过名称查询到服务，甚至是无头服务 Pod 的 IP 地址。 `P334`

服务的 IP 地址在集群每个容器的 `/etc/reslv.conf` 文件的 `nameserver` 中定义。 `coredns` Pod 利用 API 服务器的监听机制来订阅 Service 和 Endpoints 的变动，以及 DNS 记录的变更，使得其客户端相对能获取到最新的 DNS 信息。 `P334`

#### Ingress 控制器如何工作 `P334`

Ingress 控制器运行一个反向代理服务器（如 Nginx 等），根据集群中定义的 Ingress, Service 以及 Endpoints 资源来配置该控制器。所以需要订阅这些资源，然后每次其中一个发生变化则更新代理服务器的配置。 `P334`

尽管 Ingress 资源的定义指向一个 Service ， Ingress 控制器会直接将流量转发到服务的 Pod 而不经过服务 IP 。当外部客户端通过 Ingress 控制器连接时，会对客户端 IP 进行保存，这使得在某些用例中，控制器比 Service 更受欢迎。 `P335`


## 控制器如何协作 `P335`

我们创建一个 Deployment 资源，然后观察启动 Pod 的容器会发生什么。 `P335`

### 了解涉及哪些组件 `P335`

在启动整个流程之前，控制器、调度器、 Kubelet 就已经通过 API 服务器监听它们各自资源类型的变化了。 `P335`

![图 11.11 Kubernetes 组件通过 API 服务器监听 API 对象](img/chapter11/图%2011.11%20Kubernetes%20组件通过%20API%20服务器监听%20API%20对象.png)

### 事件链 `P336`

通过 `kubectl` 将包含 Deployment 的 YAML 文件提交到 Kubernetes 。 `kubectl` 将通过 HTTP POST 请求发送清单到 Kubernetes API 服务器。 API 服务器检查 Deployment 定义，存储到 etcd ，返回响应给 `kubectl` 。 `P336`

![图 11.12 Deployment 资源提交到 API 服务器的事件链](img/chapter11/图%2011.12%20Deployment%20资源提交到%20API%20服务器的事件链.png)

### 观察集群事件 `P337`

控制面板组件和 Kubelet 执行动作时，都会发送事件给 API 服务器。发送事件是通过创建事件资源来实现的，事件资源和其他的 Kubernetes 资源类似。每次使用 `kubectl describe` 来检查资源的时候，就能看到资源相关的事件，也可以直接用 `kubectl get events` 获取事件。 `P337`

## 了解运行中的 Pod 是什么 `P339`

`kubectl run nginx --image=nginx`: 运行单个容器的 Pod ： ` `P339`

`kubectl describe pod nginx-76df748b9-ggmmx`: 查看启动的 Pod 的详情，发现该 Pod 运行在 `minikube-m02` 节点上 `P339`

`minikube ssh --node='m02'`: 进入 `minikube-m02` 节点 `P339`

`docker ps`: 在节点中查看运行的容器，可以发现最新的两个容器几乎同时启动 `P339`

```shell script
CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS               NAMES
1193189f6b99        nginx                  "/docker-entrypoint.…"   4 seconds ago       Up 4 seconds                            k8s_nginx_nginx-76df748b9-ggmmx_default_193edcc8-8486-4e1e-baf3-2f872f7189fd_0
2daa93da7345        k8s.gcr.io/pause:3.2   "/pause"                 13 seconds ago      Up 13 seconds                           k8s_POD_nginx-76df748b9-ggmmx_default_193edcc8-8486-4e1e-baf3-2f872f7189fd_0
...
```

后启动的是我们期望的 nginx 容器，先启动的是一个附加容器。附加容器的启动命令是 `/pause` ，表明它没有做任何事情。 `P339`

附加容器将一个 Pod 所有的容器收纳在一起，它是一个基础容器，它的唯一目的就是保存所有的命名空间，该 Pod 的其他用户定义容器使用 Pod 的该基础容器的命名空间。 `P339`

![图 11.13 一个双容器 Pod 有 3 个运行的容器，共享同一个 Linux 命名空间](img/chapter11/图%2011.13%20一个双容器%20Pod%20有%203%20个运行的容器，共享同一个%20Linux%20命名空间.png)

实际的应用容器可能会挂掉并重启。当容器重启，容器需要处于与之前相同的 Linux 命名空间中。基础容器使这成为可能，因为它的生命周期和 Pod 绑定，基础容器从 Pod 被调度开始一直运行至 Pod 被删除。如果基础容器在这期间被终止，那么 Kubelet 会重新创建它以及 Pod 的所有容器。 `P340`

## 跨 Pod 网络 `P340`

每个 Pod 有自己唯一的 IP 地址，可以通过一个扁平的、非 NAT 网络和其他 Pod 通信。网络是由系统管理员或者 Container Network Interface(CNI) 插件建立的，而非 Kubernetes 本身。 `P340`

### 网络应该是什么样的 `P340`

Pod 用于通信的网络必须满足：Pod 自己认为的 IP 地址一定和其他节点认为该 Pod 拥有的 IP 地址一致（即数据包必须保持源和目的地址不变）。 `P340`

![图 11.14 Kubernetes 规定 Pod 必须通过非 NAT 网络进行连接](img/chapter11/图%2011.14%20Kubernetes%20规定%20Pod%20必须通过非%20NAT%20网络进行连接.png)

Pod 没有 NAT 使得运行在其中的应用可以自己注册在其他 Pod 中。 `P340`

### 深入了解网络工作原理 `P341`

Pod 的网络接口就是生成在基础设施容器的一些东西。 `P341`

![图 11.15 同一节点上 Pod 通过虚拟以太网接口对连接到同一个桥接](img/chapter11/图%2011.15%20同一节点上%20Pod%20通过虚拟以太网接口对连接到同一个桥接.png)

#### 同节点 Pod 通信 `P342`

基础设施容器启动之前，会为容器创建一个虚拟以太网接口对（一个 `veth` 对），其中一个接口保留在主机的命名空间中（在节点上运行 `ifconfig` 可以看到 `vethxxxxxxxx` 的条目），而另一个接口被移入容器网络命名空间，并重命名为 `eth0` 。两个虚拟接口就像管道的两端（或者说以太网电缆连接的两个网络设备）——从一端进入，另一端出来。 `P342`

主机网络命名空间的接口会绑定到容器运行时配置使用的网络桥接上。从网桥的地址段中取 IP 地址赋值给容器内的 `eth0` 接口。运行在容器内部的程序发送到 `eth0` 网络接口的任何数据，都会从主机命名空间的另一个 `veth` 接口出来，然后发送给网桥。这意味着任何连接到网桥的网络接口都可以接受该数据。 `P342`

如果 Pod A 发送网络包到 Pod B ，报文首先会经过 Pod A 的 `veth` 对到网桥，然后经过 Pod B 的 `veth` 对。所有节点上的容器都会连接到同一个网桥，意味着它们都能够互相通信。 `P342`

#### 不同节点上的 Pod 通信 `P342`

有多种连接不同节点上的网桥的方式： `P342`
- overlay 网络
- underlay 网络
- 三层路由

跨整个集群的 Pod 的 IP 地址必须是唯一的，所以跨节点的网桥必须使用非重叠地址段，防止不同节点上的 Pod 拿到同一个 IP 。 `P342`

![图 11.16 为了让不同节点上的 Pod 能够通信，网桥需要以某种方式连接](img/chapter11/图%2011.16%20为了让不同节点上的%20Pod%20能够通信，网桥需要以某种方式连接.png)

上图显示了通过三层网络支持跨两个节点 Pod 通信，节点的物理网络接口也需要连接到网桥。节点 A 的路由表需要被配置配置为图中所示，这样所有目的地为 `10.1.2.0/24` 的报文会被路由到节点 B ，同时节点 B 的路由需要被配置成图中所示，这样发送到 `10.1.1.0/24` 的包会被发送到节点 A 。 `P342`

仅当节点连接到相同网关、之间没有任何路由时上述方案有效。否则，路由器会扔包因为它们所涉及的 Pod IP 是私有的。 `P343`

无论多复杂，使用 SDN (软件定义网络) 技术可以让节点忽略底层网络拓扑，使得节点像连接到同一个网关上。从 Pod 发出的报文会被封装，通过网络发送给运行其他 Pod 的网络，然后被解封装，以原始格式传递给 Pod 。 `P343`

### 引入容器网络接口 `P343`

启动一个容器网络接口 (CNI) 可以让连接容器到网络更加方便。 `P343`

CNI 允许 Kubernetes 可配置使用任何 CNI 插件，包括： `P343`

- Calico
- Flannel
- Romana
- Weave Net
- 其他

可以访问 [Installing Addons](https://kubernetes.io/docs/concepts/cluster-administration/addons/) 了解每个插件的细节。

## 服务是如何实现的 `P344`

### 引入 `kube-proxy` `P344`

和 Service 相关的任何事情都由每个节点上运行的 `kube-proxy` 进程处理。前面提到过 `kube-proxy` 的代理模式有两种，默认使用 `iptables` 代理模式，如有需要也可以配置为 `userspace` 代理模式。 `P344`

每个 Service 有自己稳定的 IP 地址和端口。客户端（通常为 Pod ）通过连接该 IP 和端口使用服务。 IP 地址是虚拟的，没有被分配给任何网络接口，当数据包离开节点时也不会列为数据包的源或者目的 IP地址。 Service 的一个关键细节是，它们包含一个 IP/端口对（或者针对多端口 Service 有多个 IP/端口对），所以服务 IP 本身不代表任何东西。这就是为什么你不能够 `ping` 它们。 `P344`

### `kube-proxy` 如何使用 `iptables` `P344`

1. 当在 API 服务器中创建一个服务时，虚拟 IP 地址立刻就会分配给它
2. 之后很短时间内， API 服务器会通知所有运行在工作节点上的 `kube-proxy` 客户端有一个新服务已经被创建了
3. 然后每个 `kube-proxy` 都会让该服务在自己运行的节点上可寻址
    - 原理：通过建立一些 `iptables` 规则，确保每个目的地为服务的 IP/端口对的数据包被解析，目的地址被修改，这样数据包就会被重定向到支持服务的一个 Pod

除了监控 API 对 Service 的更改， `kube-proxy` 也监控对 Endpoints 对象的更改。因为 Endpoints 对象保存所有支持服务的 Pod 的 IP/端口对（一个 IP/端口对也可以指向除 Pod 之外的其他对象）。 `P344`

![图 11.17 发送到服务虚拟 IP|端口对的网络包会被修改、重定向到一个随机选择的后端 Pod](img/chapter11/图%2011.17%20发送到服务虚拟%20IP%7C端口对的网络包会被修改、重定向到一个随机选择的后端%20Pod.png)

包目的地初始设置为服务的 IP 和端口，发送到网络之前，节点 A 的内核会根据配置在该节点上的 `iptables` 规则处理数据包。内核检查数据包是否匹配任何这些 `iptables` 规则。其中有个规则规定如果有任何数据包的目的地 IP 等于 `172.30.0.1` 、目的地端口等于 `80` ，那么数据包的目的地 IP 和端口应该被替换为随机选中的 Pod 的 IP 和端口。 `P345`

## 运行高可用集群 `P346`

### 让你的应用变得高可用 `P346`

当在 Kubernetes 运行应用时，有不同的控制器来保证你的应用平滑运行，即使节点宕机也能够保持特定的规模。为了保证你的应用的高可用性，只需通过 Deployment 资源运行应用，配置合适数量的副本数，其他的交给 Kubernetes 处理即可。 `P346`

#### 运行多实例来减少宕机可能性 `P346`

如果副本不可用，那么 Deployment 会快速替换为一个新的。不可避免中间会有小段宕机时间。 `P346`

#### 对不能水平扩展的应用使用领导选举机制 `P346`

为了避免宕机，需要在运行一个活跃的应用的同时再运行一个附加的非活跃副本，通过一个快速生效的租约或者领导选举机制来确保只有一个是有效的。 `P346`

领导选举算法是一种分布式环境中多应用实例对谁是领导者达成一致的方式。 `P346`

- 领导者要么是唯一执行任务的那个，其他所有节点都在等待该领导者宕机，然后自己变成领导者
- 或者都是活跃的，但是领导者是唯一能够执行写操作的，而其他的只能读数据

### 让 Kubernetes 控制面板变得高可用 `P346`

为了使得 Kubernetes 高可用，需要运行多个主节点，即运行下述组件的多个实例： `P347`

- etcd 分布式数据存储：所有 API 对象存储于此
- API 服务器
- 控制器管理器：所有控制器运行的进程
- 调度器

![图 11.18 三主节点高可用集群](img/chapter11/图%2011.18%20三主节点高可用集群.png)

#### 运行 etcd 集群 `P347`

因为 etcd 被设计为一个分布式系统，其核心特征之一就是可以运行多个 etcd 实例，所以它做到高可用并非难事，只需要将其运行在合适数量的机器上，使得他们能够互相感知（通过在每个实例的配置中包含其他实例的列表）。具体操作流程可见 [Set up a High Availability etcd cluster with kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/) 。 `P348`

#### 运行多实例 API 服务器 `P348`

保证 API 服务器高可用更简单，因为 API 服务器是（几乎全部）无状态的（所有数据存储在 etcd 中， API 服务器不做缓存），需要多少就可以运行多少 API 服务器，它们不需要感知对方的存在。 `P348`

通常，一个 API 服务器会和每个 etcd 实例搭配，这样 etcd 实例之前就不需要任何负载均衡器，因为每个 API 服务器只和本地 etcd 实例通信。 `P348`

API 服务器需要一个负载均衡器，这样客户端（ `kubectl`, 也有可能是控制器管理器、调度器以及所有 Kubelet ）总是只连接到健康的 API 服务器实例。 `P348`

#### 确保控制器和调度器的高可用性 `P348`

控制器和调度器会积极监听集群状态，发生变更时作出相应操作，可能还会修改集群状态，多实例运行这些组件会导致它们执行同一个操作，会导致产生竞争状态，从而造成非预期影响。 `P348`

因此，当运行这些组件的多个实例时，给定时间内只有一个实例有效。这些组件都自己做了相应的处理（由 `--leader-elect` 选项控制，默认为 `true` ）。只有当成为选定的领导者时，组件才可能有效。只有领导者才会执行实际的工作，而其他实例都处于待命状态，等待当前领导者宕机。当领导者宕机，剩余的实例会选举新的领导者接管工作。这种机制确保不会同时出现有两个有效的组件做同样的工作。 `P348`

![图 11.19 只有一个控制器管理器和一个调度器有效，其他待机](img/chapter11/图%2011.19%20只有一个控制器管理器和一个调度器有效，其他待机.png)

控制器管理器和调度器可以和 API 服务器、 etcd 搭配运行，可以直接跟本地 API 服务器通信；或者也可以运行在不同的机器上，需要通过负载均衡器连接到 API 服务器。 `P348`

#### 控制面板组件使用的领导选择机制 `P349`

选举领导时不需要这些组件互相通信。领导选举机制的实现方式是在 API 服务器中创建一个资源，而且甚至不是什么特别的资源—— Endpoints 资源就可以用来达到目的（或者说是滥用）。 `P349`

使用 Endpoints 对象的原因是只要没有同名 Service 存在，就没有副作用，也可以使用其他任何资源。 `P349`

所有调度器实例都会尝试创建（之后更新）一个 Endpoints 资源，称为 `kube-scheduler` 。可以通过 `kubectl get endpoints -n kube-system kube-scheduler -o yaml` 获取其描述文件： `P349`

```yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"minikube_8e39ee83-638b-4c29-badc-16e8f5223465","leaseDurationSeconds":15,"acquireTime":"2020-09-13T08:16:53Z","renewTime":"2020-09-13T08:20:27Z","leaderTransitions":9}'
  creationTimestamp: "2020-08-09T02:21:48Z"
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:control-plane.alpha.kubernetes.io/leader: {}
    manager: kube-scheduler
    operation: Update
    time: "2020-09-13T08:20:27Z"
  name: kube-scheduler
  namespace: kube-system
  resourceVersion: "587818"
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler
  uid: b37fd3fb-0f69-4083-ba43-a3e32bc1447a
```

`control-plane.alpha.kubernetes.io/leader` 注解是比较重要的部分，其中包含了一个叫作 `holderIdentity` 的字段，包含了当前领导者的名字。第一个成功将名字写入该字段的实例将成为领导者，然后它会定期更新资源（默认每 2 秒），这样所有其他的实例就会知道它是否还存活。当领导者宕机，其他实例会发现资源有一段时间没更新了，就会尝试将自己的名字写到资源中尝试成为领导者。 `P349`
